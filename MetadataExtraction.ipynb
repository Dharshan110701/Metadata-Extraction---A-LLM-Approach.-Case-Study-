{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e539c-add2-4a70-9a6a-69c4639c94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b85a492-62cf-4494-a6fe-a4c9c8c9c7fa",
   "metadata": {},
   "source": [
    "## Metadata Extraction - Case Study 01\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a5780-0403-4258-aec8-cfe36092689f",
   "metadata": {},
   "source": [
    "### Importing the required packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c610fc3-f884-4925-80fa-08017b3d3ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# import pandas as pd\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0748c5-7cfb-4474-8232-c7ae88b4b2db",
   "metadata": {},
   "source": [
    "### Structure of Metadata \n",
    "The initial schema was in the form of csv which is converted in json and a detailed decription for better parsing as an input to LLM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f27dcac-85f0-46c7-a72f-bccc47c6ff4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "schema_file = \"metadata_schema.json\" \n",
    "\n",
    "# Load schema as dictionary\n",
    "with open(schema_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_schema = json.load(f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49416ac1-f883-4f2b-9be6-76812800f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a29ffc-5c5f-4417-9e12-91b99bf0edfa",
   "metadata": {},
   "source": [
    "### Structure Of Transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9a4817-b9d6-474b-8106-b4bbbd25b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"Transcripts\"\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path) & filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "        print(f\"\\nFile Loaded!\")\n",
    "input_data        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f692d37-b6a7-4f43-91f3-4a2562d886f0",
   "metadata": {},
   "source": [
    "# Loading the Model\n",
    "\n",
    "This cell describes how the \"Llama-3.3-70B-Instruct\" model is loaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c49e2-100b-49f8-8a00-942b0c094492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "model_path = \"models/meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True  # Add this for some custom models\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "metadata_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ef9be-4317-4800-afa2-00a889e9683e",
   "metadata": {},
   "source": [
    "# Transcript Chunking\n",
    "1. **Split Text into Paragraphs**\n",
    "    - We first break the transcript into paragraphs (or small chunks) so that each paragraph focuses on a relatively coherent piece of text.\n",
    "2. **Compute Paragraph Similarities**\n",
    "    - We convert each paragraph into a TF-IDF vector.\n",
    "    - We then calculate the cosine similarity between each consecutive pair of paragraphs.\n",
    "3. **Detect Dips (Topic Boundaries)**\n",
    "    - Whenever the similarity between paragraph *i* and paragraph *i+1* falls **below a chosen threshold**, we mark that as a boundary.\n",
    "    - You can pick this threshold empirically (e.g., 0.3 or 0.4), or use local minima in the similarity scores.\n",
    "4. **Create Final Segments**\n",
    "    - Merge paragraphs from one boundary to the next into a single segment.\n",
    "    - Each segment should then represent a coherent portion of the text before a significant topic shift occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aa75ef-896f-4c0f-8fd1-8b95dc236081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_paragraphs(text, sentences_per_paragraph=10):\n",
    "    \"\"\"\n",
    "    Splits the text into paragraphs containing a fixed number of sentences.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    paragraphs = []\n",
    "    for i in range(0, len(sentences), sentences_per_paragraph):\n",
    "        paragraphs.append(\" \".join(sentences[i:i+sentences_per_paragraph]))\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff53c4d-dcee-4e40-87d0-f17c9b7dcbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text_by_local_minima(text, sentences_per_paragraph=10):\n",
    "    \"\"\"\n",
    "    Segments a transcript into chunks by detecting local minima in cosine similarity\n",
    "    between adjacent paragraphs. Returns a DataFrame with one column 'Segment'\n",
    "    containing each segment's text.\n",
    "    \"\"\"\n",
    "    # Split text into paragraphs\n",
    "    paragraphs = split_into_paragraphs(text, sentences_per_paragraph)\n",
    "    \n",
    "    # If there's only one paragraph, return it as the only segment\n",
    "    if len(paragraphs) <= 1:\n",
    "        return pd.DataFrame({\"Segment\": [text]})\n",
    "    \n",
    "    # Vectorize paragraphs using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(paragraphs)\n",
    "    \n",
    "    # Compute cosine similarity between consecutive paragraphs\n",
    "    similarities = []\n",
    "    for i in range(tfidf_matrix.shape[0] - 1):\n",
    "        sim = cosine_similarity(tfidf_matrix[i], tfidf_matrix[i+1])[0][0]\n",
    "        similarities.append(sim)\n",
    "    \n",
    "    # Identify local minima: where the similarity is less than its immediate neighbors\n",
    "    boundaries = [0]  # starting index of first segment\n",
    "    for i in range(1, len(similarities) - 1):\n",
    "        if similarities[i] < similarities[i-1] and similarities[i] < similarities[i+1]:\n",
    "            boundaries.append(i + 1)\n",
    "    boundaries.append(len(paragraphs))  # end boundary\n",
    "    \n",
    "    # Build segments from boundaries\n",
    "    segments = []\n",
    "    for start, end in zip(boundaries, boundaries[1:]):\n",
    "        segment_text = \" \".join(paragraphs[start:end])\n",
    "        segments.append(segment_text)\n",
    "    \n",
    "    return pd.DataFrame({\"Segment\": segments})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1e7f34-c1fa-486a-9135-ea9b0ed6a05b",
   "metadata": {},
   "source": [
    "# Metadata Extraction with Context\n",
    "\n",
    "1. **Schema Loading and Parsing**  \n",
    "   - The function accepts a metadata schema either as a dictionary or as a file path.  \n",
    "   - If a file path is provided, it loads and parses the JSON schema, setting the structure for the metadata to be extracted.\n",
    "\n",
    "2. **Segment Processing**  \n",
    "   - Iterates through each row of a DataFrame containing transcript segments and their associated timestamp ranges.  \n",
    "   - For every segment, a detailed prompt is constructed that includes a description of the extraction task, previously aggregated context, the JSON-formatted metadata schema, the transcript text, and its timestamp.\n",
    "\n",
    "3. **Context-Aware Metadata Extraction**  \n",
    "   - The prompt is fed to a text-generation pipeline that returns a response expected to contain a JSON object with the extracted metadata.  \n",
    "   - A regular expression is used to isolate the JSON from the response, which is then parsed into a Python dictionary.\n",
    "\n",
    "4. **Aggregating Context**  \n",
    "   - Valid metadata values (those not marked as \"???\") are appended to an aggregated context string.  \n",
    "   - This cumulative context is injected into subsequent prompts to guide the extraction process by leveraging information from previous segments.\n",
    "\n",
    "5. **Final Output Construction**  \n",
    "   - The metadata from each segment is collected into a list and then converted into a pandas DataFrame.  \n",
    "   - This DataFrame provides a structured and consolidated view of all extracted metadata across the transcript segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743ea37c-8b66-4336-b184-923a54a02864",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_with_context(metadata_pipeline, segments_df: pd.DataFrame, metadata_schema: dict) -> pd.DataFrame:\n",
    "    all_metadata = []\n",
    "    \n",
    "    if isinstance(metadata_schema, str):\n",
    "        try:\n",
    "            with open(metadata_schema, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata_schema = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading metadata schema: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    schema_json = json.dumps(metadata_schema, indent=2)\n",
    "    \n",
    "    aggregated_context = \"\"\n",
    "    # print(\"Initial Aggregated Context:\")\n",
    "    # print(aggregated_context)\n",
    "    \n",
    "    for i, row in segments_df.iterrows():\n",
    "        segment_text = row[\"Segment\"]\n",
    "        # print(f\"\\nProcessing Segment {i+1}:\\n{segment_text}\\n\")\n",
    "        \n",
    "        prompt =  f\"\"\"\n",
    "        You are an AI model specialized in extracting structured metadata from German interview transcripts.\n",
    "        Below you have an interview transcript. Please analyze the interview transcript from the interviewee’s perspective.\n",
    "        The provided transcript covers the interviewee's life experiences, including positions held, work-related events, and family interactions.\n",
    "        Extract all relevant details from the text.\n",
    "        For any detected date with a two-digit year, convert it to a four-digit year by assuming it falls in the 1900s (e.g., \"67\" becomes \"1967\").\n",
    "        You will receive a JSON metadata object with predefined keys as input and values as the description of the keys.\n",
    "        Carefully analyze the transcript and extract detailed information to populate as many keys as possible.\n",
    "        Only assign '???' to a key if there is no explicit evidence in the transcript for that field.\n",
    "        Return only a valid JSON object containing the keys with their corresponding extracted values.\n",
    "        Do not include any additional text, markdown formatting, or explanations.\n",
    "                        \n",
    "        Aggregated Context from previous segments:\n",
    "        {aggregated_context}\n",
    "\n",
    "        Metadata Schema:\n",
    "        {schema_json}\n",
    "\n",
    "        Transcript:\n",
    "        {segment_text}\n",
    "\n",
    "        JSON-Antwort:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response_text = metadata_pipeline(prompt, max_new_tokens=1800, return_full_text=False, temperature=0.2)[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error generating response for segment {i+1}: {e}\")\n",
    "            response_text = \"[]\"  # Default to an empty JSON\n",
    "            \n",
    "        \n",
    "        print(f\"\\n🔍 DEBUG: Raw AI Response for segment {i+1}:\\n{response_text}\\n\")\n",
    "        response_text = response_text.strip().strip(\"[]\")\n",
    "        \n",
    "        json_match = re.search(r\"\\{[\\s\\S]*\\}\", response_text)\n",
    "        extracted_metadata = {}\n",
    "        if json_match:\n",
    "            json_str = json_match.group(0)\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                extracted_metadata.update(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON parsing error: {e}\")\n",
    "                # fallback: use line-splitting or an empty dict\n",
    "        else:\n",
    "            print(\"No JSON object found in the response.\")\n",
    "            # fallback: or just continue\n",
    "        \n",
    "        # 2. Update aggregator\n",
    "        for key in metadata_schema.keys():\n",
    "            # If the key is present and not \"???\", add it to aggregator\n",
    "            val = extracted_metadata.get(key, \"???\")\n",
    "            if val != \"???\":\n",
    "                aggregated_context += f\"{key}: {val}\\n\"\n",
    "                \n",
    "        \n",
    "        # 3. Print aggregator\n",
    "        # print(f\"\\nAggregated Context after segment {i+1}:\\n{aggregated_context}\\n\")\n",
    "        \n",
    "        # 4. Also store metadata in a list for a final DataFrame\n",
    "        # extracted_metadata[\"Segment\"] = segment_text\n",
    "        extracted_metadata = {\n",
    "            k: (str(v).replace('\"', \"\") if str(v).replace('\"', \"\").replace(\",\", \"\") != \"???\" else \"\")\n",
    "            for k, v in extracted_metadata.items()\n",
    "        }        \n",
    "        all_metadata.append(extracted_metadata)\n",
    "    return pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97b0dde-194b-426e-a735-e3dc5774bae0",
   "metadata": {},
   "source": [
    "# Main Function for Metadata Extraction from Transcripts\n",
    "\n",
    "This main function consolidates the entire workflow for processing transcript files, extracting structured metadata using an AI model, and aggregating the results into a final DataFrame. The process is as follows:\n",
    "\n",
    "1. **Schema Loading:**  \n",
    "   The metadata schema is loaded from a JSON file. This schema defines the expected keys and structure for the metadata to be extracted from the transcripts.\n",
    "\n",
    "2. **File Iteration and Data Loading:**  \n",
    "   The function iterates through all CSV files in the \"Transcripts\" folder. For each file, it reads the transcript data and extracts relevant columns such as \"Transkript\" and \"Timecode\".\n",
    "\n",
    "3. **Transcript Segmentation:**  \n",
    "   The transcript is segmented into smaller chunks using the `segment_text_by_local_minima` function, which identifies topic boundaries based on text similarity. A subset of these segments is processed to demonstrate metadata extraction.\n",
    "\n",
    "4. **Context-Aware Metadata Extraction:**  \n",
    "   The `extract_metadata_with_context` function is invoked for each segment subset. This function builds a detailed prompt incorporating both the current transcript segment and previously aggregated context, enabling the AI model to extract detailed metadata.\n",
    "\n",
    "5. **Aggregation of Metadata:**  \n",
    "   For each processed file, only the aggregated metadata (from the final segment) is collected. All metadata are then combined into a final DataFrame, providing a structured overview of the extracted information.\n",
    "\n",
    "This approach ensures that metadata is not only extracted accurately from each segment but also benefits from contextual continuity across transcript segments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc66082-0f6a-487e-9064-9b53af2ba975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main pipeline for processing transcript CSV files:\n",
    "    \n",
    "    1. Loads a metadata schema from a JSON file.\n",
    "    2. Iterates through CSV files in the 'Transcripts' folder.\n",
    "    3. For each file, loads transcript data and segments it using a local minima method.\n",
    "    4. Extracts metadata with context from the segmented transcript using an AI pipeline.\n",
    "    5. Aggregates the metadata from all processed files into a final DataFrame.\n",
    "    \n",
    "    Returns:\n",
    "        final_metadata_df (pd.DataFrame): DataFrame containing aggregated metadata.\n",
    "    \"\"\"\n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "    \n",
    "    folder_path = \"Transcripts\"\n",
    "    schema_file = \"metadata_schema.json\"\n",
    "\n",
    "    # Load the metadata schema from JSON file\n",
    "    with open(schema_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata_schema = json.load(f)\n",
    "\n",
    "    all_metadata = []  # List to store metadata DataFrames for each file\n",
    "\n",
    "    # Iterate over each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(file_path) and filename.endswith(\".csv\"):\n",
    "            print(f\"\\nProcessing file: {filename}\")\n",
    "            input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "            print(\"File Loaded!\")\n",
    "            \n",
    "            # Extract transcript and timestamp lists (if needed elsewhere)\n",
    "            transcript_text = \" \".join(input_data[\"Transkript\"].dropna().astype(str))\n",
    "            segments_df = segment_text_by_local_minima(transcript_text, sentences_per_paragraph=10)\n",
    "            segments_df_subset = segments_df.head(3)\n",
    "            llama_70b_responses = extract_metadata_with_context(metadata_pipeline, segments_df_subset, metadata_schema)\n",
    "\n",
    "            if not llama_70b_responses.empty:\n",
    "            # Taking only the last row as it has the aggregated values\n",
    "                metadata_extracted = llama_70b_responses.tail(1)\n",
    "                metadata_extracted[\"Filename\"] = filename\n",
    "                all_metadata.append(metadata_extracted)\n",
    "            \n",
    "            else:\n",
    "                print(f\"No metadata extracted from {filename}\")\n",
    "         \n",
    "        time.sleep(0.5)  # Optional pause between processing files\n",
    "\n",
    "    # Combine all metadata into a single DataFrame\n",
    "    if all_metadata:\n",
    "        final_metadata_df = pd.concat(all_metadata, ignore_index=True)\n",
    "    else:\n",
    "        final_metadata_df = pd.DataFrame()\n",
    "    \n",
    "    # print(\"\\nFinal Metadata DataFrame:\")\n",
    "    # print(final_metadata_df)\n",
    "    return final_metadata_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    final_metadata_df = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be58d08-7eca-4b52-8f02-0cd3efed7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4da0b9-3d8e-4fda-9389-78abddae0a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
