{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b35cdaf-9582-4d07-96dd-601567b447a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#IMPORT STATEMENTS\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcbf3616-9e22-46d1-b3d5-13f81f927c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Ensure 'Transkript' column has no NaN values\n",
    "    data['Transkript'] = data['Transkript'].fillna(\"\").astype(str)\n",
    "\n",
    "    # Chunking logic\n",
    "    chunked_data = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        speaker = row['Sprecher']\n",
    "        transcript = row['Transkript']\n",
    "\n",
    "        if speaker != current_speaker:\n",
    "            # Save previous chunk if exists\n",
    "            if current_speaker is not None:\n",
    "                chunked_data.append({\n",
    "                    'Speaker': current_speaker,\n",
    "                    'Transcript': current_text.strip(),\n",
    "                })\n",
    "            \n",
    "            # Start a new chunk\n",
    "            current_speaker = speaker\n",
    "            current_text = transcript\n",
    "        else:\n",
    "            # Continue appending to the same speaker's chunk\n",
    "            current_text += \" \" + transcript if isinstance(transcript, str) else \"\"\n",
    "\n",
    "    # Save the last chunk\n",
    "    if current_speaker is not None:\n",
    "        chunked_data.append({\n",
    "            'Speaker': current_speaker,\n",
    "            'Transcript': current_text.strip(),\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    chunked_df = pd.DataFrame(chunked_data)\n",
    "    \n",
    "    return chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "deee020a-14b6-4de1-b9cb-58980f35d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentence(chunked_df: pd.DataFrame, min_tokens=256, max_tokens=512) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Further splits chunks by sentence while ensuring each chunk is within a token range.\n",
    "\n",
    "    Args:\n",
    "        chunked_df (pd.DataFrame): Input DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        min_tokens (int): Minimum number of tokens per chunk.\n",
    "        max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with sentence-based chunked transcripts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to count tokens (approximate, assuming 1 word ‚âà 1.2 tokens)\n",
    "    def count_tokens(text):\n",
    "        return len(text.split()) * 1.2  # Rough estimate\n",
    "\n",
    "    # Initialize list for final merged chunks\n",
    "    merged_chunks = []\n",
    "    temp_chunk = []\n",
    "    temp_token_count = 0\n",
    "    speaker = None\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in chunked_df.iterrows():\n",
    "        sentence = row['Transcript']\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        # If adding this chunk keeps us within MAX_TOKENS\n",
    "        if temp_token_count + sentence_tokens <= max_tokens:\n",
    "            if not temp_chunk:\n",
    "                speaker = row['Speaker']  # Store speaker only for new chunks\n",
    "            temp_chunk.append(sentence)\n",
    "            temp_token_count += sentence_tokens\n",
    "        else:\n",
    "            # Save the previous chunk before starting a new one\n",
    "            if temp_chunk:\n",
    "                merged_chunks.append({\n",
    "                    'Speaker': speaker,\n",
    "                    'Transcript': \" \".join(temp_chunk),\n",
    "                })\n",
    "\n",
    "            # Start a new chunk with the current sentence\n",
    "            temp_chunk = [sentence]\n",
    "            temp_token_count = sentence_tokens\n",
    "            speaker = row['Speaker']\n",
    "\n",
    "\n",
    "    # Save last chunk if any content remains\n",
    "    if temp_chunk:\n",
    "        merged_chunks.append({\n",
    "            'Speaker': speaker,\n",
    "            'Transcript': \" \".join(temp_chunk)\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_merged_df = pd.DataFrame(merged_chunks)\n",
    "    \n",
    "    return final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c273c4fc-b034-44fc-b754-24df59b30995",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92adebc69331404eaf2487fc61a49b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\n",
    "\n",
    "model_path = \"models/meta-llama/Llama-3.3-70B-Instruct\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    quantization_config=quantization_config,\n",
    "    trust_remote_code=True  # Add this for some custom models\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "metadata_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9688527d-162f-44db-a7d1-3b8550ddbb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_metadate(metadata_pipeline, chunks: pd.DataFrame, metadata_schema: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from multiple chunks of a transcript using the specified model.\n",
    "\n",
    "    Args:\n",
    "        client: The AI client object.\n",
    "        model_name: The AI model to use.\n",
    "        chunks: A DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        metadata_schema: A dictionary representing the metadata schema.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A structured DataFrame containing extracted metadata.\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "\n",
    "    if isinstance(metadata_schema, str):\n",
    "        try:\n",
    "            with open(metadata_schema, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata_schema = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading metadata schema: {e}\")\n",
    "            return pd.DataFrame()\n",
    "                \n",
    "    schema_json = json.dumps(metadata_schema, indent=2)\n",
    "\n",
    "    for i, row in chunks.iloc[:5].iterrows():\n",
    "        speaker = row[\"Speaker\"]\n",
    "        transcript = row[\"Transcript\"]  # Prevent AI truncation\n",
    "\n",
    "\n",
    "\n",
    "          # JSON-structured prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI model specialized in extracting structured metadata from interview transcripts. \n",
    "        Return **only a valid JSON object** based on the given schema. \n",
    "        If information is missing, set the value to `???`. \n",
    "        Do not add explanations, markdown formatting, or extra text.\n",
    "\n",
    "        Erforderliches Format:\n",
    "        {schema_json}\n",
    "\n",
    "        Transkript:\n",
    "        {transcript}\n",
    "\n",
    "        JSON-Antwort:\n",
    "        \"\"\"\n",
    "\n",
    "         # Generate response using the local LLaMA model\n",
    "        try:\n",
    "            response_text =  metadata_pipeline(prompt, max_new_tokens=1800,return_full_text=False,temperature=0.2)[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating response for chunk {i+1}: {e}\")\n",
    "            response_text = \"{}\"  # Default to an empty JSON\n",
    "\n",
    "        # Debugging: Print the raw AI response to identify issues\n",
    "        print(f\"\\nüîç DEBUG: Raw AI Response for chunk {i+1}:\\n{response_text}\\n\")\n",
    "        \n",
    "\n",
    "\n",
    "    return extracted_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c74d92e9-7610-4094-b8f2-43838595c4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: adg0001_er_2024_10_31.csv\n",
      "\n",
      "üîç DEBUG: Raw AI Response for chunk 1:\n",
      " [\n",
      "           {\n",
      "             \"Standort\": \"Hemer im Sauerland\",\n",
      "             \"Archiv ID\": \"???\",\n",
      "             \"PROBANDNR\": \"???\",\n",
      "             \"DOK_ART\": \"???\",\n",
      "             \"ARCHIVORT\": \"???\",\n",
      "             \"PROVENIENZ\": \"???\",\n",
      "             \"SPERRUNG\": \"???\",\n",
      "             \"ENTSTZEIT\": \"???\",\n",
      "             \"Zeitumfang 1\": \"???\",\n",
      "             \"NAME\": \"???\",\n",
      "             \"VORNAME\": \"???\",\n",
      "             \"ORT\": \"Hemer im Sauerland\",\n",
      "             \"Feld1\": \"???\",\n",
      "             \"PSEUDONYM\": \"???\",\n",
      "             \"GESCHLECHT\": \"???\",\n",
      "             \"JAHRGANG\": \"1925\",\n",
      "             \"IPV\": \"???\",\n",
      "             \"DATENBOGEN\": \"???\",\n",
      "             \"KURZBESCHR\": \"???\",\n",
      "             \"TITEL\": \"???\",\n",
      "             \"STRASSE\": \"???\",\n",
      "             \"PLZ\": \"???\",\n",
      "             \"TELEFON\": \"???\",\n",
      "             \"GRUPPE\": \"???\",\n",
      "             \"BERUF\": \"???\",\n",
      "             \"HEUT_FAMST\": \"???\",\n",
      "             \"INTERVIEWE\": \"???\",\n",
      "             \"TIPPER\": \"???\",\n",
      "             \"Segmentierung\": \"???\",\n",
      "             \"DATUM1\": \"???\",\n",
      "             \"DATUM2\": \"???\",\n",
      "             \"DATUM3\": \"???\",\n",
      "             \"DAUER\": \"???\",\n",
      "             \"online\": \"???\",\n",
      "             \"AUSDRUCKSART\": \"???\",\n",
      "             \"UNKAUSDRUC\": \"???\",\n",
      "             \"KORRAUSDRU\": \"???\",\n",
      "             \"SCHLAGWORT\": \"???\",\n",
      "             \"KURZBIOGRA\": \"???\",\n",
      "             \"KURZPROTOK\": \"???\",\n",
      "             \"FOTOS\": \"???\",\n",
      "             \"DOKUMENTE\": \"???\",\n",
      "             \"VHS\": \"???\",\n",
      "             \"DVD\": \"???\",\n",
      "             \"IBM Server\": \"???\",\n",
      "             \"Cloud\": \"???\",\n",
      "             \"Format Cloud\": \"???\",\n",
      "             \"DV\": \"???\",\n",
      "             \"Beta\": \"???\",\n",
      "             \"ORIGCASSET\": \"???\",\n",
      "             \"CASSKOPIEN\": \"???\",\n",
      "             \"FESTPLATTE\": \"???\",\n",
      "             \"Dig Audiofiles\": \"???\",\n",
      "             \"KONF_HEUTE\": \"???\",\n",
      "             \"KONVERSION\": \"???\",\n",
      "             \"WANN_KONV\": \"???\",\n",
      "             \"HERKUNFT\": \"Hemer im Sauerland\",\n",
      "             \"WANN_ZUGEZ\": \"???\",\n",
      "             \"GESCHWISTE\": \"???\",\n",
      "             \"Schulabsch\": \"Hauptschulabschluss\",\n",
      "             \"ABGEBROCHE\": \"???\",\n",
      "             \"WEITERBILD\": \"???\",\n",
      "             \"AUSBILDUNG\": \"Landjahr-Lager\",\n",
      "             \"STAND\": \"???\",\n",
      "             \"WIRTSCHBER\": \"???\",\n",
      "             \"BERUFSWECH\": \"???\",\n",
      "             \"WANN_WECHS\": \"???\",\n",
      "             \"AUFABSTIEG\": \"???\",\n",
      "             \"BERUFSBEGI\": \"???\",\n",
      "             \"BERUFSENDE\": \"???\",\n",
      "             \"NICHTERWER\": \"???\",\n",
      "             \"GRNDE\": \"???\",\n",
      "             \"VON_BIS\": \"???\",\n",
      "             \"ARBEITSLOS\": \"???\",\n",
      "             \"VON_BIS_AL\": \"???\",\n",
      "             \"FAM_STAND\": \"???\",\n",
      "             \"HEIRAT1JHR\": \"???\",\n",
      "             \"HEIRAT2JHR\": \"???\",\n",
      "             \"HEIRAT3JHR\": \"???\",\n",
      "             \"SCHEID1JHR\": \"???\",\n",
      "             \"SCHEID2JHR\": \"???\",\n",
      "             \"VERWIT1JHR\": \"???\",\n",
      "             \"VERWIT2JHR\": \"???\",\n",
      "             \"KINDERZAHL\": \"???\",\n",
      "             \"GEB_JAHR1\": \"???\",\n",
      "             \"GEB_JAHR2\": \"???\",\n",
      "             \"GEB_JAHR_L\": \"???\",\n",
      "             \"AUFABKIND\": \"???\",\n",
      "             \"POLOR_HEUT\": \"???\",\n",
      "             \"POL_KONVER\": \"???\",\n",
      "             \"POLORIENT1\": \"???\",\n",
      "             \"VON_BIS_1\": \"???\",\n",
      "             \"POLORIENT2\": \"???\",\n",
      "             \"VON_BIS_2\": \"???\",\n",
      "             \"GEW_VERBAN\": \"???\",\n",
      "             \"VON_BIS_GV\": \"???\",\n",
      "             \"JUGENDORG1\": \"???\",\n",
      "             \"VON_BIS_J1\": \"???\",\n",
      "             \"JUGENDORG2\": \"???\",\n",
      "             \"VON_BIS_J2\": \"???\",\n",
      "             \"NS_ORGAN_1\": \"???\",\n",
      "             \"VON_BISNS1\": \"???\",\n",
      "             \"NS_ORGAN_2\": \"???\",\n",
      "             \"VON_BISNS2\": \"???\",\n",
      "             \"RAD_KLV_DV\": \"???\",\n",
      "             \"VON_BISRAD\": \"???\",\n",
      "             \"SONST_ENG\": \"???\",\n",
      "             \"VON_BIS_SE\": \"???\",\n",
      "             \"KRIEGSTEIL\": \"???\",\n",
      "             \"VON_BIS_KR\": \"???\",\n",
      "             \"BES_BERICH\": \"???\",\n",
      "             \"MUTT_JG\": \"???\",\n",
      "             \"MUTT_KONFESSION\": \"???\",\n",
      "             \"MUTT_HERKU\": \"???\",\n",
      "             \"MUTT_SCHUL\": \"???\",\n",
      "             \"MUTT_AUSBI\": \"???\",\n",
      "             \"MUTT_STAND\": \"???\",\n",
      "             \"MUTT_POLOR\": \"???\",\n",
      "             \"VAT_JG\": \"???\",\n",
      "             \"VAT_KONFESSION\": \"???\",\n",
      "             \"VAT_HERKUN\": \"???\",\n",
      "             \"VAT_SCHULE\": \"???\",\n",
      "             \"VAT_AUSBIL\": \"???\",\n",
      "             \"VAT_STAND\": \"???\",\n",
      "             \"VAT_POLOR\": \"???\",\n",
      "             \"PART_JG\": \"???\",\n",
      "             \"PART_KONFESSION\": \"???\",\n",
      "             \"PART_HERKU\": \"???\",\n",
      "             \"PART_SCHUL\": \"???\",\n",
      "             \"PART_AUSBI\": \"???\",\n",
      "             \"PART_STAND\": \"???\",\n",
      "             \"PART_BERUF\": \"???\",\n",
      "             \"PART_POLOR\": \"???\",\n",
      "             \"PART_PKONV\": \"???\",\n",
      "             \"PART_ENGAG\": \"???\",\n",
      "             \"KRIT10\": \"???\"\n",
      "           }\n",
      "         ]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m final_chunks_df \u001b[38;5;241m=\u001b[39m chunk_by_sentence(speaker_chunks_df)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(final_chunks_df)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Extract metadata for the chunks\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m llama_70b_responses \u001b[38;5;241m=\u001b[39m \u001b[43mextract_metadate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetadata_pipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_chunks_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Ensure that the response DataFrame contains metadata columns\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m llama_70b_responses\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Merge chunk outputs into a single row \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 54\u001b[0m, in \u001b[0;36mextract_metadate\u001b[0;34m(metadata_pipeline, chunks, metadata_schema)\u001b[0m\n\u001b[1;32m     52\u001b[0m  \u001b[38;5;66;03m# Generate response using the local LLaMA model\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     response_text \u001b[38;5;241m=\u001b[39m  \u001b[43mmetadata_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1800\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreturn_full_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚ùå Error generating response for chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py:272\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(chats, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1268\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1261\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[1;32m   1266\u001b[0m     )\n\u001b[1;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1274\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1275\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1276\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/base.py:1175\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1174\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1175\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_generation.py:370\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    368\u001b[0m     generate_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration_config\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[0;32m--> 370\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:2047\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2039\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2040\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2041\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2042\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2043\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2044\u001b[0m     )\n\u001b[1;32m   2046\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2047\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2048\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2050\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2051\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2052\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2058\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2059\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2060\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2061\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2066\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2067\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:3043\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3041\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3042\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3043\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3045\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "folder_path = \"Transcripts\"\n",
    "schema_file = \"metadata_schema.json\" \n",
    "# Load schema as dictionary\n",
    "with open(schema_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_schema = json.load(f) \n",
    "    \n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# List to store metadata for all files\n",
    "all_metadata = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path) & filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "        speaker_chunks_df = chunk_transcript(input_data)  # Stores speaker-based chunks\n",
    "        final_chunks_df = chunk_by_sentence(speaker_chunks_df)\n",
    "        # print(final_chunks_df)\n",
    "        \n",
    "\n",
    "        # Extract metadata for the chunks\n",
    "        llama_70b_responses = extract_metadate(metadata_pipeline, final_chunks_df, metadata_schema)\n",
    "        # Ensure that the response DataFrame contains metadata columns\n",
    "        if not llama_70b_responses.empty:\n",
    "            # Merge chunk outputs into a single row \n",
    "            merged_metadata = llama_70b_responses.apply(lambda col: ' '.join(col.dropna().astype(str)))\n",
    "            \n",
    "            for column in merged_metadata.index:\n",
    "                unique_values = set([value.strip() for value in merged_metadata[column].strip().split(\",\")])\n",
    "                list_unique_values = list(filter(None, unique_values))\n",
    "                merged_metadata[column] = \" | \".join(list_unique_values)\n",
    "\n",
    "            # Add filename for reference\n",
    "            # merged_metadata[\"Filename\"] = filename  \n",
    "\n",
    "            # Append to list\n",
    "            all_metadata.append(merged_metadata)\n",
    "        else:\n",
    "            print(f\"No metadata extracted from {filename}\")\n",
    "         \n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert list of metadata rows into a single DataFrame\n",
    "final_metadata_df = pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08a149a5-a4d2-4101-93a6-6b89e4a5900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_metadate(metadata_pipeline, chunks: pd.DataFrame, metadata_schema: dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from multiple chunks of a transcript using the specified model.\n",
    "\n",
    "    Args:\n",
    "        client: The AI client object.\n",
    "        model_name: The AI model to use.\n",
    "        chunks: A DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        metadata_schema: A dictionary representing the metadata schema.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A structured DataFrame containing extracted metadata.\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "\n",
    "    if isinstance(metadata_schema, str):\n",
    "        try:\n",
    "            with open(metadata_schema, \"r\", encoding=\"utf-8\") as f:\n",
    "                metadata_schema = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading metadata schema: {e}\")\n",
    "            return pd.DataFrame()\n",
    "                \n",
    "    schema_json = json.dumps(metadata_schema, indent=2)\n",
    "\n",
    "    for i, row in chunks.iloc[:5].iterrows():\n",
    "        speaker = row[\"Speaker\"]\n",
    "        transcript = row[\"Transcript\"]  # Prevent AI truncation\n",
    "\n",
    "\n",
    "\n",
    "          # JSON-structured prompt\n",
    "        prompt = f\"\"\"\n",
    "        You are an AI model specialized in extracting structured metadata from interview transcripts. \n",
    "        Return **only a valid JSON object** based on the given schema. \n",
    "        If information is missing, set the value to `???`. \n",
    "        Do not add explanations, markdown formatting, or extra text.\n",
    "\n",
    "        Erforderliches Format:\n",
    "        {schema_json}\n",
    "\n",
    "        Transkript:\n",
    "        {transcript}\n",
    "\n",
    "        JSON-Antwort:\n",
    "        \"\"\"\n",
    "\n",
    "         # Generate response using the local LLaMA model\n",
    "        try:\n",
    "            response_text =  metadata_pipeline(prompt, max_new_tokens=1800,return_full_text=False,temperature=0.2)[0][\"generated_text\"]\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating response for chunk {i+1}: {e}\")\n",
    "            response_text = \"{}\"  # Default to an empty JSON\n",
    "\n",
    "        # Debugging: Print the raw AI response to identify issues\n",
    "        print(f\"\\nüîç DEBUG: Raw AI Response for chunk {i+1}:\\n{response_text}\\n\")\n",
    "        \n",
    "        response_text = response_text.strip(\"[]\")  # Handle array brackets if any\n",
    "        try:\n",
    "            metadata = json.loads(response_text)\n",
    "            all_metadata.append(metadata)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ö†Ô∏è JSON decoding error in chunk {i+1}: {e}\")\n",
    "            all_metadata.append({})  # Append empty JSON if parsing fails\n",
    "\n",
    "    return all_metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3e92c1e-d8ab-4555-b86c-90776d423477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: adg0001_er_2024_10_31.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç DEBUG: Raw AI Response for chunk 1:\n",
      " [\n",
      "           {\n",
      "             \"Standort\": \"Hemer im Sauerland\",\n",
      "             \"Archiv ID\": \"???\",\n",
      "             \"PROBANDNR\": \"???\",\n",
      "             \"DOK_ART\": \"???\",\n",
      "             \"ARCHIVORT\": \"???\",\n",
      "             \"PROVENIENZ\": \"???\",\n",
      "             \"SPERRUNG\": \"???\",\n",
      "             \"ENTSTZEIT\": \"???\",\n",
      "             \"Zeitumfang 1\": \"???\",\n",
      "             \"NAME\": \"???\",\n",
      "             \"VORNAME\": \"???\",\n",
      "             \"ORT\": \"Hemer im Sauerland\",\n",
      "             \"Feld1\": \"???\",\n",
      "             \"PSEUDONYM\": \"???\",\n",
      "             \"GESCHLECHT\": \"???\",\n",
      "             \"JAHRGANG\": \"1925\",\n",
      "             \"IPV\": \"???\",\n",
      "             \"DATENBOGEN\": \"???\",\n",
      "             \"KURZBESCHR\": \"???\",\n",
      "             \"TITEL\": \"???\",\n",
      "             \"STRASSE\": \"???\",\n",
      "             \"PLZ\": \"???\",\n",
      "             \"TELEFON\": \"???\",\n",
      "             \"GRUPPE\": \"???\",\n",
      "             \"BERUF\": \"???\",\n",
      "             \"HEUT_FAMST\": \"???\",\n",
      "             \"INTERVIEWE\": \"???\",\n",
      "             \"TIPPER\": \"???\",\n",
      "             \"Segmentierung\": \"???\",\n",
      "             \"DATUM1\": \"???\",\n",
      "             \"DATUM2\": \"???\",\n",
      "             \"DATUM3\": \"???\",\n",
      "             \"DAUER\": \"???\",\n",
      "             \"online\": \"???\",\n",
      "             \"AUSDRUCKSART\": \"???\",\n",
      "             \"UNKAUSDRUC\": \"???\",\n",
      "             \"KORRAUSDRU\": \"???\",\n",
      "             \"SCHLAGWORT\": \"???\",\n",
      "             \"KURZBIOGRA\": \"???\",\n",
      "             \"KURZPROTOK\": \"???\",\n",
      "             \"FOTOS\": \"???\",\n",
      "             \"DOKUMENTE\": \"???\",\n",
      "             \"VHS\": \"???\",\n",
      "             \"DVD\": \"???\",\n",
      "             \"IBM Server\": \"???\",\n",
      "             \"Cloud\": \"???\",\n",
      "             \"Format Cloud\": \"???\",\n",
      "             \"DV\": \"???\",\n",
      "             \"Beta\": \"???\",\n",
      "             \"ORIGCASSET\": \"???\",\n",
      "             \"CASSKOPIEN\": \"???\",\n",
      "             \"FESTPLATTE\": \"???\",\n",
      "             \"Dig Audiofiles\": \"???\",\n",
      "             \"KONF_HEUTE\": \"???\",\n",
      "             \"KONVERSION\": \"???\",\n",
      "             \"WANN_KONV\": \"???\",\n",
      "             \"HERKUNFT\": \"Hemer im Sauerland\",\n",
      "             \"WANN_ZUGEZ\": \"???\",\n",
      "             \"GESCHWISTE\": \"???\",\n",
      "             \"Schulabsch\": \"Hauptschulabschluss\",\n",
      "             \"ABGEBROCHE\": \"???\",\n",
      "             \"WEITERBILD\": \"???\",\n",
      "             \"AUSBILDUNG\": \"Landjahr-Lager\",\n",
      "             \"STAND\": \"???\",\n",
      "             \"WIRTSCHBER\": \"???\",\n",
      "             \"BERUFSWECH\": \"???\",\n",
      "             \"WANN_WECHS\": \"???\",\n",
      "             \"AUFABSTIEG\": \"???\",\n",
      "             \"BERUFSBEGI\": \"???\",\n",
      "             \"BERUFSENDE\": \"???\",\n",
      "             \"NICHTERWER\": \"???\",\n",
      "             \"GRNDE\": \"???\",\n",
      "             \"VON_BIS\": \"???\",\n",
      "             \"ARBEITSLOS\": \"???\",\n",
      "             \"VON_BIS_AL\": \"???\",\n",
      "             \"FAM_STAND\": \"???\",\n",
      "             \"HEIRAT1JHR\": \"???\",\n",
      "             \"HEIRAT2JHR\": \"???\",\n",
      "             \"HEIRAT3JHR\": \"???\",\n",
      "             \"SCHEID1JHR\": \"???\",\n",
      "             \"SCHEID2JHR\": \"???\",\n",
      "             \"VERWIT1JHR\": \"???\",\n",
      "             \"VERWIT2JHR\": \"???\",\n",
      "             \"KINDERZAHL\": \"???\",\n",
      "             \"GEB_JAHR1\": \"???\",\n",
      "             \"GEB_JAHR2\": \"???\",\n",
      "             \"GEB_JAHR_L\": \"???\",\n",
      "             \"AUFABKIND\": \"???\",\n",
      "             \"POLOR_HEUT\": \"???\",\n",
      "             \"POL_KONVER\": \"???\",\n",
      "             \"POLORIENT1\": \"???\",\n",
      "             \"VON_BIS_1\": \"???\",\n",
      "             \"POLORIENT2\": \"???\",\n",
      "             \"VON_BIS_2\": \"???\",\n",
      "             \"GEW_VERBAN\": \"???\",\n",
      "             \"VON_BIS_GV\": \"???\",\n",
      "             \"JUGENDORG1\": \"???\",\n",
      "             \"VON_BIS_J1\": \"???\",\n",
      "             \"JUGENDORG2\": \"???\",\n",
      "             \"VON_BIS_J2\": \"???\",\n",
      "             \"NS_ORGAN_1\": \"???\",\n",
      "             \"VON_BISNS1\": \"???\",\n",
      "             \"NS_ORGAN_2\": \"???\",\n",
      "             \"VON_BISNS2\": \"???\",\n",
      "             \"RAD_KLV_DV\": \"???\",\n",
      "             \"VON_BISRAD\": \"???\",\n",
      "             \"SONST_ENG\": \"???\",\n",
      "             \"VON_BIS_SE\": \"???\",\n",
      "             \"KRIEGSTEIL\": \"???\",\n",
      "             \"VON_BIS_KR\": \"???\",\n",
      "             \"BES_BERICH\": \"???\",\n",
      "             \"MUTT_JG\": \"???\",\n",
      "             \"MUTT_KONFESSION\": \"???\",\n",
      "             \"MUTT_HERKU\": \"???\",\n",
      "             \"MUTT_SCHUL\": \"???\",\n",
      "             \"MUTT_AUSBI\": \"???\",\n",
      "             \"MUTT_STAND\": \"???\",\n",
      "             \"MUTT_POLOR\": \"???\",\n",
      "             \"VAT_JG\": \"???\",\n",
      "             \"VAT_KONFESSION\": \"???\",\n",
      "             \"VAT_HERKUN\": \"???\",\n",
      "             \"VAT_SCHULE\": \"???\",\n",
      "             \"VAT_AUSBIL\": \"???\",\n",
      "             \"VAT_STAND\": \"???\",\n",
      "             \"VAT_POLOR\": \"???\",\n",
      "             \"PART_JG\": \"???\",\n",
      "             \"PART_KONFESSION\": \"???\",\n",
      "             \"PART_HERKU\": \"???\",\n",
      "             \"PART_SCHUL\": \"???\",\n",
      "             \"PART_AUSBI\": \"???\",\n",
      "             \"PART_STAND\": \"???\",\n",
      "             \"PART_BERUF\": \"???\",\n",
      "             \"PART_POLOR\": \"???\",\n",
      "             \"PART_PKONV\": \"???\",\n",
      "             \"PART_ENGAG\": \"???\",\n",
      "             \"KRIT10\": \"???\"\n",
      "           }\n",
      "         ]\n",
      "\n",
      "‚ö†Ô∏è JSON decoding error in chunk 1: Expecting ',' delimiter: line 140 column 10 (char 4589)\n",
      "\n",
      "üîç DEBUG: Raw AI Response for chunk 2:\n",
      " [\n",
      "           {\n",
      "             \"Standort\": \"???\",\n",
      "             \"Archiv ID\": \"???\",\n",
      "             \"PROBANDNR\": \"???\",\n",
      "             \"DOK_ART\": \"???\",\n",
      "             \"ARCHIVORT\": \"???\",\n",
      "             \"PROVENIENZ\": \"???\",\n",
      "             \"SPERRUNG\": \"???\",\n",
      "             \"ENTSTZEIT\": \"???\",\n",
      "             \"Zeitumfang 1\": \"???\",\n",
      "             \"NAME\": \"???\",\n",
      "             \"VORNAME\": \"???\",\n",
      "             \"ORT\": \"???\",\n",
      "             \"Feld1\": \"???\",\n",
      "             \"PSEUDONYM\": \"???\",\n",
      "             \"GESCHLECHT\": \"???\",\n",
      "             \"JAHRGANG\": \"???\",\n",
      "             \"IPV\": \"???\",\n",
      "             \"DATENBOGEN\": \"???\",\n",
      "             \"KURZBESCHR\": \"???\",\n",
      "             \"TITEL\": \"???\",\n",
      "             \"STRASSE\": \"???\",\n",
      "             \"PLZ\": \"???\",\n",
      "             \"TELEFON\": \"???\",\n",
      "             \"GRUPPE\": \"???\",\n",
      "             \"BERUF\": \"???\",\n",
      "             \"HEUT_FAMST\": \"???\",\n",
      "             \"INTERVIEWE\": \"???\",\n",
      "             \"TIPPER\": \"???\",\n",
      "             \"Segmentierung\": \"???\",\n",
      "             \"DATUM1\": \"???\",\n",
      "             \"DATUM2\": \"???\",\n",
      "             \"DATUM3\": \"???\",\n",
      "             \"DAUER\": \"???\",\n",
      "             \"online\": \"???\",\n",
      "             \"AUSDRUCKSART\": \"???\",\n",
      "             \"UNKAUSDRUC\": \"???\",\n",
      "             \"KORRAUSDRU\": \"???\",\n",
      "             \"SCHLAGWORT\": \"???\",\n",
      "             \"KURZBIOGRA\": \"???\",\n",
      "             \"KURZPROTOK\": \"???\",\n",
      "             \"FOTOS\": \"???\",\n",
      "             \"DOKUMENTE\": \"???\",\n",
      "             \"VHS\": \"???\",\n",
      "             \"DVD\": \"???\",\n",
      "             \"IBM Server\": \"???\",\n",
      "             \"Cloud\": \"???\",\n",
      "             \"Format Cloud\": \"???\",\n",
      "             \"DV\": \"???\",\n",
      "             \"Beta\": \"???\",\n",
      "             \"ORIGCASSET\": \"???\",\n",
      "             \"CASSKOPIEN\": \"???\",\n",
      "             \"FESTPLATTE\": \"???\",\n",
      "             \"Dig Audiofiles\": \"???\",\n",
      "             \"KONF_HEUTE\": \"???\",\n",
      "             \"KONVERSION\": \"???\",\n",
      "             \"WANN_KONV\": \"???\",\n",
      "             \"HERKUNFT\": \"???\",\n",
      "             \"WANN_ZUGEZ\": \"???\",\n",
      "             \"GESCHWISTE\": \"1\",\n",
      "             \"Schulabsch\": \"???\",\n",
      "             \"ABGEBROCHE\": \"???\",\n",
      "             \"WEITERBILD\": \"???\",\n",
      "             \"AUSBILDUNG\": \"B√ºroarbeit\",\n",
      "             \"STAND\": \"???\",\n",
      "             \"WIRTSCHBER\": \"???\",\n",
      "             \"BERUFSWECH\": \"???\",\n",
      "             \"WANN_WECHS\": \"???\",\n",
      "             \"AUFABSTIEG\": \"???\",\n",
      "             \"BERUFSBEGI\": \"???\",\n",
      "             \"BERUFSENDE\": \"???\",\n",
      "             \"NICHTERWER\": \"???\",\n",
      "             \"GR√úNDE\": \"???\",\n",
      "             \"VON_BIS\": \"???\",\n",
      "             \"ARBEITSLOS\": \"???\",\n",
      "             \"VON_BIS_AL\": \"???\",\n",
      "             \"FAM_STAND\": \"???\",\n",
      "             \"HEIRAT1JHR\": \"???\",\n",
      "             \"HEIRAT2JHR\": \"???\",\n",
      "             \"HEIRAT3JHR\": \"???\",\n",
      "             \"SCHEID1JHR\": \"???\",\n",
      "             \"SCHEID2JHR\": \"???\",\n",
      "             \"VERWIT1JHR\": \"???\",\n",
      "             \"VERWIT2JHR\": \"???\",\n",
      "             \"KINDERZAHL\": \"1\",\n",
      "             \"GEB_JAHR1\": \"???\",\n",
      "             \"GEB_JAHR2\": \"???\",\n",
      "             \"GEB_JAHR_L\": \"???\",\n",
      "             \"AUFABKIND\": \"???\",\n",
      "             \"POLOR_HEUT\": \"???\",\n",
      "             \"POL_KONVER\": \"???\",\n",
      "             \"POLORIENT1\": \"???\",\n",
      "             \"VON_BIS_1\": \"???\",\n",
      "             \"POLORIENT2\": \"???\",\n",
      "             \"VON_BIS_2\": \"???\",\n",
      "             \"GEW_VERBAN\": \"???\",\n",
      "             \"VON_BIS_GV\": \"???\",\n",
      "             \"JUGENDORG1\": \"???\",\n",
      "             \"VON_BIS_J1\": \"???\",\n",
      "             \"JUGENDORG2\": \"???\",\n",
      "             \"VON_BIS_J2\": \"???\",\n",
      "             \"NS_ORGAN_1\": \"???\",\n",
      "             \"VON_BISNS1\": \"???\",\n",
      "             \"NS_ORGAN_2\": \"???\",\n",
      "             \"VON_BISNS2\": \"???\",\n",
      "             \"RAD_KLV_DV\": \"???\",\n",
      "             \"VON_BISRAD\": \"???\",\n",
      "             \"SONST_ENG\": \"???\",\n",
      "             \"VON_BIS_SE\": \"???\",\n",
      "             \"KRIEGSTEIL\": \"???\",\n",
      "             \"VON_BIS_KR\": \"???\",\n",
      "             \"BES_BERICH\": \"???\",\n",
      "             \"MUTT_JG\": \"???\",\n",
      "             \"MUTT_KONFESSION\": \"???\",\n",
      "             \"MUTT_HERKU\": \"???\",\n",
      "             \"MUTT_SCHUL\": \"???\",\n",
      "             \"MUTT_AUSBI\": \"???\",\n",
      "             \"MUTT_STAND\": \"???\",\n",
      "             \"MUTT_POLOR\": \"???\",\n",
      "             \"VAT_JG\": \"???\",\n",
      "             \"VAT_KONFESSION\": \"???\",\n",
      "             \"VAT_HERKUN\": \"???\",\n",
      "             \"VAT_SCHULE\": \"???\",\n",
      "             \"VAT_AUSBIL\": \"???\",\n",
      "             \"VAT_STAND\": \"???\",\n",
      "             \"VAT_POLOR\": \"???\",\n",
      "             \"PART_JG\": \"???\",\n",
      "             \"PART_KONFESSION\": \"???\",\n",
      "             \"PART_HERKU\": \"???\",\n",
      "             \"PART_SCHUL\": \"???\",\n",
      "             \"PART_AUSBI\": \"???\",\n",
      "             \"PART_STAND\": \"???\",\n",
      "             \"PART_BERUF\": \"???\",\n",
      "             \"PART_POLOR\": \"???\",\n",
      "             \"PART_PKONV\": \"???\",\n",
      "             \"PART_ENGAG\": \"???\",\n",
      "             \"KRIT10\": \"???\"\n",
      "           }\n",
      "         ]\n",
      "\n",
      "‚ö†Ô∏è JSON decoding error in chunk 2: Expecting ',' delimiter: line 140 column 10 (char 4520)\n",
      "\n",
      "üîç DEBUG: Raw AI Response for chunk 3:\n",
      " [\n",
      "  {\n",
      "    \"Standort\": \"M√ºlheim an der M√∂hne\",\n",
      "    \"Archiv ID\": \"???\",\n",
      "    \"PROBANDNR\": \"???\",\n",
      "    \"DOK_ART\": \"???\",\n",
      "    \"ARCHIVORT\": \"???\",\n",
      "    \"PROVENIENZ\": \"???\",\n",
      "    \"SPERRUNG\": \"???\",\n",
      "    \"ENTSTZEIT\": \"???\",\n",
      "    \"Zeitumfang 1\": \"???\",\n",
      "    \"NAME\": \"???\",\n",
      "    \"VORNAME\": \"???\",\n",
      "    \"ORT\": \"M√ºlheim an der M√∂hne\",\n",
      "    \"Feld1\": \"???\",\n",
      "    \"PSEUDONYM\": \"???\",\n",
      "    \"GESCHLECHT\": \"???\",\n",
      "    \"JAHRGANG\": \"???\",\n",
      "    \"IPV\": \"???\",\n",
      "    \"DATENBOGEN\": \"???\",\n",
      "    \"KURZBESCHR\": \"???\",\n",
      "    \"TITEL\": \"???\",\n",
      "    \"STRASSE\": \"???\",\n",
      "    \"PLZ\": \"???\",\n",
      "    \"TELEFON\": \"???\",\n",
      "    \"GRUPPE\": \"NSV, Arbeitsdienst\",\n",
      "    \"BERUF\": \"???\",\n",
      "    \"HEUT_FAMST\": \"???\",\n",
      "    \"INTERVIEWE\": \"???\",\n",
      "    \"TIPPER\": \"???\",\n",
      "    \"Segmentierung\": \"???\",\n",
      "    \"DATUM1\": \"November 1942\",\n",
      "    \"DATUM2\": \"???\",\n",
      "    \"DATUM3\": \"???\",\n",
      "    \"DAUER\": \"???\",\n",
      "    \"online\": \"???\",\n",
      "    \"AUSDRUCKSART\": \"???\",\n",
      "    \"UNKAUSDRUC\": \"???\",\n",
      "    \"KORRAUSDRU\": \"???\",\n",
      "    \"SCHLAGWORT\": \"???\",\n",
      "    \"KURZBIOGRA\": \"???\",\n",
      "    \"KURZPROTOK\": \"???\",\n",
      "    \"FOTOS\": \"???\",\n",
      "    \"DOKUMENTE\": \"???\",\n",
      "    \"VHS\": \"???\",\n",
      "    \"DVD\": \"???\",\n",
      "    \"IBM Server\": \"???\",\n",
      "    \"Cloud\": \"???\",\n",
      "    \"Format Cloud\": \"???\",\n",
      "    \"DV\": \"???\",\n",
      "    \"Beta\": \"???\",\n",
      "    \"ORIGCASSET\": \"???\",\n",
      "    \"CASSKOPIEN\": \"???\",\n",
      "    \"FESTPLATTE\": \"???\",\n",
      "    \"Dig Audiofiles\": \"???\",\n",
      "    \"KONF_HEUTE\": \"???\",\n",
      "    \"KONVERSION\": \"???\",\n",
      "    \"WANN_KONV\": \"???\",\n",
      "    \"HERKUNFT\": \"???\",\n",
      "    \"WANN_ZUGEZ\": \"???\",\n",
      "    \"GESCHWISTE\": \"???\",\n",
      "    \"Schulabsch\": \"???\",\n",
      "    \"ABGEBROCHE\": \"???\",\n",
      "    \"WEITERBILD\": \"???\",\n",
      "    \"AUSBILDUNG\": \"???\",\n",
      "    \"STAND\": \"???\",\n",
      "    \"WIRTSCHBER\": \"???\",\n",
      "    \"BERUFSWECH\": \"???\",\n",
      "    \"WANN_WECHS\": \"???\",\n",
      "    \"AUFABSTIEG\": \"???\",\n",
      "    \"BERUFSBEGI\": \"???\",\n",
      "    \"BERUFSENDE\": \"???\",\n",
      "    \"NICHTERWER\": \"???\",\n",
      "    \"GR√úNDE\": \"???\",\n",
      "    \"VON_BIS\": \"???\",\n",
      "    \"ARBEITSLOS\": \"???\",\n",
      "    \"VON_BIS_AL\": \"???\",\n",
      "    \"FAM_STAND\": \"???\",\n",
      "    \"HEIRAT1JHR\": \"???\",\n",
      "    \"HEIRAT2JHR\": \"???\",\n",
      "    \"HEIRAT3JHR\": \"???\",\n",
      "    \"SCHEID1JHR\": \"???\",\n",
      "    \"SCHEID2JHR\": \"???\",\n",
      "    \"VERWIT1JHR\": \"???\",\n",
      "    \"VERWIT2JHR\": \"???\",\n",
      "    \"KINDERZAHL\": \"???\",\n",
      "    \"GEB_JAHR1\": \"???\",\n",
      "    \"GEB_JAHR2\": \"???\",\n",
      "    \"GEB_JAHR_L\": \"???\",\n",
      "    \"AUFABKIND\": \"???\",\n",
      "    \"POLOR_HEUT\": \"???\",\n",
      "    \"POL_KONVER\": \"???\",\n",
      "    \"POLORIENT1\": \"???\",\n",
      "    \"VON_BIS_1\": \"???\",\n",
      "    \"POLORIENT2\": \"???\",\n",
      "    \"VON_BIS_2\": \"???\",\n",
      "    \"GEW_VERBAN\": \"???\",\n",
      "    \"VON_BIS_GV\": \"???\",\n",
      "    \"JUGENDORG1\": \"???\",\n",
      "    \"VON_BIS_J1\": \"???\",\n",
      "    \"JUGENDORG2\": \"???\",\n",
      "    \"VON_BIS_J2\": \"???\",\n",
      "    \"NS_ORGAN_1\": \"???\",\n",
      "    \"VON_BISNS1\": \"???\",\n",
      "    \"NS_ORGAN_2\": \"???\",\n",
      "    \"VON_BISNS2\": \"???\",\n",
      "    \"RAD_KLV_DV\": \"Ja\",\n",
      "    \"VON_BISRAD\": \"November 1942\",\n",
      "    \"SONST_ENG\": \"???\",\n",
      "    \"VON_BIS_SE\": \"???\",\n",
      "    \"KRIEGSTEIL\": \"???\",\n",
      "    \"VON_BIS_KR\": \"???\",\n",
      "    \"BES_BERICH\": \"???\",\n",
      "    \"MUTT_JG\": \"???\",\n",
      "    \"MUTT_KONFESSION\": \"???\",\n",
      "    \"MUTT_HERKU\": \"???\",\n",
      "    \"MUTT_SCHUL\": \"???\",\n",
      "    \"MUTT_AUSBI\": \"???\",\n",
      "    \"MUTT_STAND\": \"???\",\n",
      "    \"MUTT_POLOR\": \"???\",\n",
      "    \"VAT_JG\": \"???\",\n",
      "    \"VAT_KONFESSION\": \"???\",\n",
      "    \"VAT_HERKUN\": \"???\",\n",
      "    \"VAT_SCHULE\": \"???\",\n",
      "    \"VAT_AUSBIL\": \"???\",\n",
      "    \"VAT_STAND\": \"???\",\n",
      "    \"VAT_POLOR\": \"???\",\n",
      "    \"PART_JG\": \"???\",\n",
      "    \"PART_KONFESSION\": \"???\",\n",
      "    \"PART_HERKU\": \"???\",\n",
      "    \"PART_SCHUL\": \"???\",\n",
      "    \"PART_AUSBI\": \"???\",\n",
      "    \"PART_STAND\": \"???\",\n",
      "    \"PART_BERUF\": \"???\",\n",
      "    \"PART_POLOR\": \"???\",\n",
      "    \"PART_PKONV\": \"???\",\n",
      "    \"PART_ENGAG\": \"???\",\n",
      "    \"KRIT10\": \"???\"\n",
      "  }\n",
      "]\n",
      "\n",
      "‚ö†Ô∏è JSON decoding error in chunk 3: Expecting ',' delimiter: line 140 column 1 (char 3334)\n",
      "\n",
      "üîç DEBUG: Raw AI Response for chunk 4:\n",
      " [\n",
      "           {\n",
      "             \"Standort\": \"???\",\n",
      "             \"Archiv ID\": \"???\",\n",
      "             \"PROBANDNR\": \"???\",\n",
      "             \"DOK_ART\": \"???\",\n",
      "             \"ARCHIVORT\": \"???\",\n",
      "             \"PROVENIENZ\": \"???\",\n",
      "             \"SPERRUNG\": \"???\",\n",
      "             \"ENTSTZEIT\": \"???\",\n",
      "             \"Zeitumfang 1\": \"???\",\n",
      "             \"NAME\": \"???\",\n",
      "             \"VORNAME\": \"???\",\n",
      "             \"ORT\": \"???\",\n",
      "             \"Feld1\": \"???\",\n",
      "             \"PSEUDONYM\": \"???\",\n",
      "             \"GESCHLECHT\": \"???\",\n",
      "             \"JAHRGANG\": \"???\",\n",
      "             \"IPV\": \"???\",\n",
      "             \"DATENBOGEN\": \"???\",\n",
      "             \"KURZBESCHR\": \"???\",\n",
      "             \"TITEL\": \"???\",\n",
      "             \"STRASSE\": \"???\",\n",
      "             \"PLZ\": \"???\",\n",
      "             \"TELEFON\": \"???\",\n",
      "             \"GRUPPE\": \"???\",\n",
      "             \"BERUF\": \"???\",\n",
      "             \"HEUT_FAMST\": \"???\",\n",
      "             \"INTERVIEWE\": \"???\",\n",
      "             \"TIPPER\": \"???\",\n",
      "             \"Segmentierung\": \"???\",\n",
      "             \"DATUM1\": \"???\",\n",
      "             \"DATUM2\": \"???\",\n",
      "             \"DATUM3\": \"???\",\n",
      "             \"DAUER\": \"???\",\n",
      "             \"online\": \"???\",\n",
      "             \"AUSDRUCKSART\": \"???\",\n",
      "             \"UNKAUSDRUC\": \"???\",\n",
      "             \"KORRAUSDRU\": \"???\",\n",
      "             \"SCHLAGWORT\": \"???\",\n",
      "             \"KURZBIOGRA\": \"???\",\n",
      "             \"KURZPROTOK\": \"???\",\n",
      "             \"FOTOS\": \"???\",\n",
      "             \"DOKUMENTE\": \"???\",\n",
      "             \"VHS\": \"???\",\n",
      "             \"DVD\": \"???\",\n",
      "             \"IBM Server\": \"???\",\n",
      "             \"Cloud\": \"???\",\n",
      "             \"Format Cloud\": \"???\",\n",
      "             \"DV\": \"???\",\n",
      "             \"Beta\": \"???\",\n",
      "             \"ORIGCASSET\": \"???\",\n",
      "             \"CASSKOPIEN\": \"???\",\n",
      "             \"FESTPLATTE\": \"???\",\n",
      "             \"Dig Audiofiles\": \"???\",\n",
      "             \"KONF_HEUTE\": \"???\",\n",
      "             \"KONVERSION\": \"???\",\n",
      "             \"WANN_KONV\": \"???\",\n",
      "             \"HERKUNFT\": \"???\",\n",
      "             \"WANN_ZUGEZ\": \"???\",\n",
      "             \"GESCHWISTE\": \"???\",\n",
      "             \"Schulabsch\": \"???\",\n",
      "             \"ABGEBROCHE\": \"???\",\n",
      "             \"WEITERBILD\": \"???\",\n",
      "             \"AUSBILDUNG\": \"???\",\n",
      "             \"STAND\": \"???\",\n",
      "             \"WIRTSCHBER\": \"???\",\n",
      "             \"BERUFSWECH\": \"???\",\n",
      "             \"WANN_WECHS\": \"???\",\n",
      "             \"AUFABSTIEG\": \"???\",\n",
      "             \"BERUFSBEGI\": \"???\",\n",
      "             \"BERUFSENDE\": \"???\",\n",
      "             \"NICHTERWER\": \"???\",\n",
      "             \"GR\\u00dcNDE\": \"???\",\n",
      "             \"VON_BIS\": \"???\",\n",
      "             \"ARBEITSLOS\": \"???\",\n",
      "             \"VON_BIS_AL\": \"???\",\n",
      "             \"FAM_STAND\": \"???\",\n",
      "             \"HEIRAT1JHR\": \"???\",\n",
      "             \"HEIRAT2JHR\": \"???\",\n",
      "             \"HEIRAT3JHR\": \"???\",\n",
      "             \"SCHEID1JHR\": \"???\",\n",
      "             \"SCHEID2JHR\": \"???\",\n",
      "             \"VERWIT1JHR\": \"???\",\n",
      "             \"VERWIT2JHR\": \"???\",\n",
      "             \"KINDERZAHL\": \"???\",\n",
      "             \"GEB_JAHR1\": \"???\",\n",
      "             \"GEB_JAHR2\": \"???\",\n",
      "             \"GEB_JAHR_L\": \"???\",\n",
      "             \"AUFABKIND\": \"???\",\n",
      "             \"POLOR_HEUT\": \"???\",\n",
      "             \"POL_KONVER\": \"???\",\n",
      "             \"POLORIENT1\": \"???\",\n",
      "             \"VON_BIS_1\": \"???\",\n",
      "             \"POLORIENT2\": \"???\",\n",
      "             \"VON_BIS_2\": \"???\",\n",
      "             \"GEW_VERBAN\": \"???\",\n",
      "             \"VON_BIS_GV\": \"???\",\n",
      "             \"JUGENDORG1\": \"???\",\n",
      "             \"VON_BIS_J1\": \"???\",\n",
      "             \"JUGENDORG2\": \"???\",\n",
      "             \"VON_BIS_J2\": \"???\",\n",
      "             \"NS_ORGAN_1\": \"???\",\n",
      "             \"VON_BISNS1\": \"???\",\n",
      "             \"NS_ORGAN_2\": \"???\",\n",
      "             \"VON_BISNS2\": \"???\",\n",
      "             \"RAD_KLV_DV\": \"???\",\n",
      "             \"VON_BISRAD\": \"???\",\n",
      "             \"SONST_ENG\": \"???\",\n",
      "             \"VON_BIS_SE\": \"???\",\n",
      "             \"KRIEGSTEIL\": \"???\",\n",
      "             \"VON_BIS_KR\": \"???\",\n",
      "             \"BES_BERICH\": \"???\",\n",
      "             \"MUTT_JG\": \"???\",\n",
      "             \"MUTT_KONFESSION\": \"???\",\n",
      "             \"MUTT_HERKU\": \"???\",\n",
      "             \"MUTT_SCHUL\": \"???\",\n",
      "             \"MUTT_AUSBI\": \"???\",\n",
      "             \"MUTT_STAND\": \"???\",\n",
      "             \"MUTT_POLOR\": \"???\",\n",
      "             \"VAT_JG\": \"???\",\n",
      "             \"VAT_KONFESSION\": \"???\",\n",
      "             \"VAT_HERKUN\": \"???\",\n",
      "             \"VAT_SCHULE\": \"???\",\n",
      "             \"VAT_AUSBIL\": \"???\",\n",
      "             \"VAT_STAND\": \"???\",\n",
      "             \"VAT_POLOR\": \"???\",\n",
      "             \"PART_JG\": \"???\",\n",
      "             \"PART_KONFESSION\": \"???\",\n",
      "             \"PART_HERKU\": \"???\",\n",
      "             \"PART_SCHUL\": \"???\",\n",
      "             \"PART_AUSBI\": \"???\",\n",
      "             \"PART_STAND\": \"???\",\n",
      "             \"PART_BERUF\": \"???\",\n",
      "             \"PART_POLOR\": \"???\",\n",
      "             \"PART_PKONV\": \"???\",\n",
      "             \"PART_ENGAG\": \"???\",\n",
      "             \"KRIT10\": \"???\"\n",
      "           }\n",
      "         ]\n",
      "\n",
      "‚ö†Ô∏è JSON decoding error in chunk 4: Expecting ',' delimiter: line 140 column 10 (char 4522)\n",
      "\n",
      "üîç DEBUG: Raw AI Response for chunk 5:\n",
      " [\n",
      "           {\n",
      "             \"Standort\": \"???\",\n",
      "             \"Archiv ID\": \"???\",\n",
      "             \"PROBANDNR\": \"???\",\n",
      "             \"DOK_ART\": \"???\",\n",
      "             \"ARCHIVORT\": \"???\",\n",
      "             \"PROVENIENZ\": \"???\",\n",
      "             \"SPERRUNG\": \"???\",\n",
      "             \"ENTSTZEIT\": \"???\",\n",
      "             \"Zeitumfang 1\": \"???\",\n",
      "             \"NAME\": \"???\",\n",
      "             \"VORNAME\": \"???\",\n",
      "             \"ORT\": \"???\",\n",
      "             \"Feld1\": \"???\",\n",
      "             \"PSEUDONYM\": \"???\",\n",
      "             \"GESCHLECHT\": \"???\",\n",
      "             \"JAHRGANG\": \"???\",\n",
      "             \"IPV\": \"???\",\n",
      "             \"DATENBOGEN\": \"???\",\n",
      "             \"KURZBESCHR\": \"???\",\n",
      "             \"TITEL\": \"???\",\n",
      "             \"STRASSE\": \"???\",\n",
      "             \"PLZ\": \"???\",\n",
      "             \"TELEFON\": \"???\",\n",
      "             \"GRUPPE\": \"???\",\n",
      "             \"BERUF\": \"???\",\n",
      "             \"HEUT_FAMST\": \"???\",\n",
      "             \"INTERVIEWE\": \"???\",\n",
      "             \"TIPPER\": \"???\",\n",
      "             \"Segmentierung\": \"???\",\n",
      "             \"DATUM1\": \"???\",\n",
      "             \"DATUM2\": \"???\",\n",
      "             \"DATUM3\": \"???\",\n",
      "             \"DAUER\": \"???\",\n",
      "             \"online\": \"???\",\n",
      "             \"AUSDRUCKSART\": \"???\",\n",
      "             \"UNKAUSDRUC\": \"???\",\n",
      "             \"KORRAUSDRU\": \"???\",\n",
      "             \"SCHLAGWORT\": \"???\",\n",
      "             \"KURZBIOGRA\": \"???\",\n",
      "             \"KURZPROTOK\": \"???\",\n",
      "             \"FOTOS\": \"???\",\n",
      "             \"DOKUMENTE\": \"???\",\n",
      "             \"VHS\": \"???\",\n",
      "             \"DVD\": \"???\",\n",
      "             \"IBM Server\": \"???\",\n",
      "             \"Cloud\": \"???\",\n",
      "             \"Format Cloud\": \"???\",\n",
      "             \"DV\": \"???\",\n",
      "             \"Beta\": \"???\",\n",
      "             \"ORIGCASSET\": \"???\",\n",
      "             \"CASSKOPIEN\": \"???\",\n",
      "             \"FESTPLATTE\": \"???\",\n",
      "             \"Dig Audiofiles\": \"???\",\n",
      "             \"KONF_HEUTE\": \"???\",\n",
      "             \"KONVERSION\": \"???\",\n",
      "             \"WANN_KONV\": \"???\",\n",
      "             \"HERKUNFT\": \"???\",\n",
      "             \"WANN_ZUGEZ\": \"???\",\n",
      "             \"GESCHWISTE\": \"???\",\n",
      "             \"Schulabsch\": \"???\",\n",
      "             \"ABGEBROCHE\": \"???\",\n",
      "             \"WEITERBILD\": \"???\",\n",
      "             \"AUSBILDUNG\": \"???\",\n",
      "             \"STAND\": \"???\",\n",
      "             \"WIRTSCHBER\": \"???\",\n",
      "             \"BERUFSWECH\": \"???\",\n",
      "             \"WANN_WECHS\": \"???\",\n",
      "             \"AUFABSTIEG\": \"???\",\n",
      "             \"BERUFSBEGI\": \"???\",\n",
      "             \"BERUFSENDE\": \"???\",\n",
      "             \"NICHTERWER\": \"???\",\n",
      "             \"GR\\u00dcNDE\": \"???\",\n",
      "             \"VON_BIS\": \"???\",\n",
      "             \"ARBEITSLOS\": \"???\",\n",
      "             \"VON_BIS_AL\": \"???\",\n",
      "             \"FAM_STAND\": \"???\",\n",
      "             \"HEIRAT1JHR\": \"???\",\n",
      "             \"HEIRAT2JHR\": \"???\",\n",
      "             \"HEIRAT3JHR\": \"???\",\n",
      "             \"SCHEID1JHR\": \"???\",\n",
      "             \"SCHEID2JHR\": \"???\",\n",
      "             \"VERWIT1JHR\": \"???\",\n",
      "             \"VERWIT2JHR\": \"???\",\n",
      "             \"KINDERZAHL\": \"???\",\n",
      "             \"GEB_JAHR1\": \"???\",\n",
      "             \"GEB_JAHR2\": \"???\",\n",
      "             \"GEB_JAHR_L\": \"???\",\n",
      "             \"AUFABKIND\": \"???\",\n",
      "             \"POLOR_HEUT\": \"???\",\n",
      "             \"POL_KONVER\": \"???\",\n",
      "             \"POLORIENT1\": \"???\",\n",
      "             \"VON_BIS_1\": \"???\",\n",
      "             \"POLORIENT2\": \"???\",\n",
      "             \"VON_BIS_2\": \"???\",\n",
      "             \"GEW_VERBAN\": \"???\",\n",
      "             \"VON_BIS_GV\": \"???\",\n",
      "             \"JUGENDORG1\": \"???\",\n",
      "             \"VON_BIS_J1\": \"???\",\n",
      "             \"JUGENDORG2\": \"???\",\n",
      "             \"VON_BIS_J2\": \"???\",\n",
      "             \"NS_ORGAN_1\": \"???\",\n",
      "             \"VON_BISNS1\": \"???\",\n",
      "             \"NS_ORGAN_2\": \"???\",\n",
      "             \"VON_BISNS2\": \"???\",\n",
      "             \"RAD_KLV_DV\": \"???\",\n",
      "             \"VON_BISRAD\": \"???\",\n",
      "             \"SONST_ENG\": \"???\",\n",
      "             \"VON_BIS_SE\": \"???\",\n",
      "             \"KRIEGSTEIL\": \"???\",\n",
      "             \"VON_BIS_KR\": \"???\",\n",
      "             \"BES_BERICH\": \"???\",\n",
      "             \"MUTT_JG\": \"???\",\n",
      "             \"MUTT_KONFESSION\": \"???\",\n",
      "             \"MUTT_HERKU\": \"???\",\n",
      "             \"MUTT_SCHUL\": \"???\",\n",
      "             \"MUTT_AUSBI\": \"???\",\n",
      "             \"MUTT_STAND\": \"???\",\n",
      "             \"MUTT_POLOR\": \"???\",\n",
      "             \"VAT_JG\": \"???\",\n",
      "             \"VAT_KONFESSION\": \"???\",\n",
      "             \"VAT_HERKUN\": \"???\",\n",
      "             \"VAT_SCHULE\": \"???\",\n",
      "             \"VAT_AUSBIL\": \"???\",\n",
      "             \"VAT_STAND\": \"???\",\n",
      "             \"VAT_POLOR\": \"???\",\n",
      "             \"PART_JG\": \"???\",\n",
      "             \"PART_KONFESSION\": \"???\",\n",
      "             \"PART_HERKU\": \"???\",\n",
      "             \"PART_SCHUL\": \"???\",\n",
      "             \"PART_AUSBI\": \"???\",\n",
      "             \"PART_STAND\": \"???\",\n",
      "             \"PART_BERUF\": \"???\",\n",
      "             \"PART_POLOR\": \"???\",\n",
      "             \"PART_PKONV\": \"???\",\n",
      "             \"PART_ENGAG\": \"???\",\n",
      "             \"KRIT10\": \"???\"\n",
      "           }\n",
      "         ]\n",
      "\n",
      "‚ö†Ô∏è JSON decoding error in chunk 5: Expecting ',' delimiter: line 140 column 10 (char 4522)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "folder_path = \"Transcripts\"\n",
    "schema_file = \"metadata_schema.json\"\n",
    "\n",
    "# Load schema as dictionary\n",
    "with open(schema_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    metadata_schema = json.load(f)\n",
    "\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# List to store metadata for all files\n",
    "all_metadata = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    if os.path.isfile(file_path) and filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "\n",
    "        # Load transcript file\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "\n",
    "        # Chunk transcript (assuming these functions exist)\n",
    "        speaker_chunks_df = chunk_transcript(input_data)  \n",
    "        final_chunks_df = chunk_by_sentence(speaker_chunks_df)\n",
    "\n",
    "        # Extract metadata for the chunks\n",
    "        llama_70b_responses = extract_metadate(metadata_pipeline, final_chunks_df, metadata_schema)\n",
    "\n",
    "        if llama_70b_responses:  # Check if list is not empty\n",
    "            merged_metadata = {}\n",
    "\n",
    "            # Iterate over extracted metadata (list of JSON objects)\n",
    "            for response in llama_70b_responses:\n",
    "                for key, value in response.items():\n",
    "                    if key not in merged_metadata:\n",
    "                        merged_metadata[key] = set()  # Use a set to store unique values\n",
    "\n",
    "                    # Ensure value is a string before processing\n",
    "                    if isinstance(value, list):\n",
    "                        value = \", \".join(str(v) for v in value)\n",
    "                    elif not isinstance(value, str):\n",
    "                        value = str(value)\n",
    "\n",
    "                    # Add to the set\n",
    "                    merged_metadata[key].add(value.strip())\n",
    "\n",
    "            # Convert set values to a string separated by \" | \"\n",
    "            merged_metadata = {key: \" | \".join(sorted(values)) for key, values in merged_metadata.items()}\n",
    "\n",
    "            # Add filename for reference\n",
    "            merged_metadata[\"Filename\"] = filename  \n",
    "\n",
    "            # Append to list\n",
    "            all_metadata.append(merged_metadata)\n",
    "        else:\n",
    "            print(f\"No metadata extracted from {filename}\")\n",
    "\n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert list of metadata into a single DataFrame\n",
    "final_metadata_df = pd.DataFrame(all_metadata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f2d0cff3-937a-4f1b-a1bd-08dc6ac37c04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adg0001_er_2024_10_31.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Filename\n",
       "0  adg0001_er_2024_10_31.csv"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metadata_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
