{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35d49b97-55a8-40f7-9861-f450ab0cb16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.66.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bf0abf-f7bd-4f31-abe7-9350fce54151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6436c012-5008-435b-89b5-658b701e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Ensure 'Transkript' column has no NaN values\n",
    "    data['Transkript'] = data['Transkript'].fillna(\"\").astype(str)\n",
    "\n",
    "    # Chunking logic\n",
    "    chunked_data = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    initial_timestamp = \"\"\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        speaker = row['Sprecher']\n",
    "        transcript = row['Transkript']\n",
    "        timestamp = row['Timecode'] \n",
    "\n",
    "        if speaker != current_speaker:\n",
    "            # Save previous chunk if exists\n",
    "            if current_speaker is not None:\n",
    "                chunked_data.append({\n",
    "                    'Speaker': current_speaker,\n",
    "                    'Transcript': current_text.strip(),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : timestamp\n",
    "                })\n",
    "            \n",
    "            # Start a new chunk\n",
    "            current_speaker = speaker\n",
    "            current_text = transcript\n",
    "            initial_timestamp = timestamp \n",
    "        else:\n",
    "            # Continue appending to the same speaker's chunk\n",
    "            current_text += \" \" + transcript if isinstance(transcript, str) else \"\"\n",
    "\n",
    "    # Save the last chunk\n",
    "    if current_speaker is not None:\n",
    "        chunked_data.append({\n",
    "            'Speaker': current_speaker,\n",
    "            'Transcript': current_text.strip(),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    chunked_df = pd.DataFrame(chunked_data)\n",
    "    \n",
    "    return chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc851cb5-4772-4693-a60f-fe27baf3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentence(chunked_df: pd.DataFrame, min_tokens=256, max_tokens=512) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Further splits chunks by sentence while ensuring each chunk is within a token range.\n",
    "\n",
    "    Args:\n",
    "        chunked_df (pd.DataFrame): Input DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        min_tokens (int): Minimum number of tokens per chunk.\n",
    "        max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with sentence-based chunked transcripts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to count tokens (approximate, assuming 1 word â‰ˆ 1.2 tokens)\n",
    "    def count_tokens(text):\n",
    "        return len(text.split()) * 1.2  # Rough estimate\n",
    "\n",
    "    # Initialize list for final merged chunks\n",
    "    merged_chunks = []\n",
    "    temp_chunk = []\n",
    "    temp_token_count = 0\n",
    "    speaker = None\n",
    "    initial_timestamp = \"\"\n",
    "    final_timestamp = \"\"\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in chunked_df.iterrows():\n",
    "        sentence = row['Transcript']\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        # If adding this chunk keeps us within MAX_TOKENS\n",
    "        if temp_token_count + sentence_tokens <= max_tokens:\n",
    "            if not temp_chunk:\n",
    "                speaker = row['Speaker']  # Store speaker only for new chunks\n",
    "                initial_timestamp = row['Initial_Timestamp']\n",
    "            temp_chunk.append(sentence)\n",
    "            temp_token_count += sentence_tokens\n",
    "        else:\n",
    "            # Save the previous chunk before starting a new one\n",
    "            if temp_chunk:\n",
    "                merged_chunks.append({\n",
    "                    'Speaker': speaker,\n",
    "                    'Transcript': \" \".join(temp_chunk),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : row['Current_Timestamp']\n",
    "                })\n",
    "\n",
    "            # Start a new chunk with the current sentence\n",
    "            temp_chunk = [sentence]\n",
    "            temp_token_count = sentence_tokens\n",
    "            speaker = row['Speaker']\n",
    "            initial_timestamp = row['Initial_Timestamp']\n",
    "            final_timestamp = row['Current_Timestamp']\n",
    "\n",
    "    # Save last chunk if any content remains\n",
    "    if temp_chunk:\n",
    "        merged_chunks.append({\n",
    "            'Speaker': speaker,\n",
    "            'Transcript': \" \".join(temp_chunk),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : final_timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_merged_df = pd.DataFrame(merged_chunks)\n",
    "    \n",
    "    return final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e7d285-cb30-4f3f-b232-debf27d00341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2827e8447b4344dc996e14a4cb86d3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"models/meta-llama/Llama-3.3-70B-Instruct\"\n",
    "quantization_config = transformers.BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "nlp_pipeline = pipeline(\"text-generation\", model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5e477f9-da20-4f79-bad0-ef5196ce1321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.pipelines.text_generation.TextGenerationPipeline at 0x7fdcadc6b4d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d45450-b828-406f-b791-96dbee91ee33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(model_name: str, chunks: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from multiple chunks of a German transcript using the specified model.\n",
    "    \n",
    "    Args:\n",
    "        client: The Groq client object.\n",
    "        model_name: The name of the model to use (e.g., 'llama3-8b-8192').\n",
    "        chunks: A DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the extracted metadata for all chunks.\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "    num_chunks = len(chunks)\n",
    "\n",
    "    for i, row in chunks.iloc[:5].iterrows():  # Iterate over DataFrame rows\n",
    "        speaker = row[\"Speaker\"]\n",
    "        transcript = row[\"Transcript\"]\n",
    "        timestamp = row['Timestamp']\n",
    "\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)} for Speaker: {speaker}...\")\n",
    "\n",
    "        # Prompt for metadata extraction\n",
    "        prompt = f\"\"\"\n",
    "Extrahieren Sie die folgenden Informationen aus dem Transkript und geben Sie die Antwort auf Deutsch ein. \n",
    "Wenn die Antwort nicht gefunden wurde, geben Sie stattdessen %%% zurÃ¼ck. \n",
    "FÃ¼gen Sie nach jedem Wert in Klammern den entsprechenden {timestamp} hinzu. \n",
    "aber nur, wenn der Wert nicht %%% ist und den Wert aus dem Kontext behalten, aber den Zeitstempel nicht hinzufÃ¼gen. FÃ¼gen Sie keine PrÃ¤ambel ein.\n",
    "\n",
    "\n",
    "Transkript:\n",
    "{transcript}\n",
    "\n",
    "Metadaten columns to be present:\n",
    "        NAME: \n",
    "        JAHRGANG: \n",
    "        ORT: \n",
    "        GESCHLECHT: \n",
    "        BERUF: \n",
    "        VAT_JG: \n",
    "        VAT_KONFESSION: \n",
    "        VAT_HERKUN: \n",
    "        VAT_SCHULE: \n",
    "        VAT_AUSBIL: \n",
    "        VAT_STAND: \n",
    "        VAT_POLOR:\n",
    "\n",
    "FÃ¼gen Sie keine PrÃ¤ambel ein.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "        # Generate response using the Groq client\n",
    "        response = nlp_pipeline(prompt, max_new_tokens=200, truncation=True)[0][\"generated_text\"]\n",
    "\n",
    "        # Extract metadata from the response\n",
    "        metadata = response\n",
    "        # Parse metadata into a dictionary\n",
    "        extracted_metadata = {\"Speaker\": speaker}  # Store speaker info\n",
    "        for line in metadata.split(\"\\n\"):\n",
    "            if \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "                extracted_metadata[key.strip()] = value.strip()\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        if i < 5 - 1:  # If not the last iteration\n",
    "            extracted_metadata = {k: v.strip() + \",\" if v else v.strip() for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Append metadata for this chunk\n",
    "        all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    return pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66dc81a9-7248-402f-aaae-5de035b0b945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file: adg0001_er_2024_10_31.csv\n",
      "Processing chunk 1/21 for Speaker: INT_AH...\n",
      "Processing chunk 2/21 for Speaker: IP_FA...\n",
      "Processing chunk 3/21 for Speaker: INT_AH...\n",
      "Processing chunk 4/21 for Speaker: IP_FA...\n",
      "Processing chunk 5/21 for Speaker: INT_AH...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "folder_path = \"Transcripts\"\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# List to store metadata for all files\n",
    "all_metadata = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path) & filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "        speaker_chunks_df = chunk_transcript(input_data)  # Stores speaker-based chunks\n",
    "        final_chunks_df = chunk_by_sentence(speaker_chunks_df)\n",
    "        final_chunks_df['Timestamp'] = final_chunks_df['Initial_Timestamp'] + \" - \" + final_chunks_df['Current_Timestamp'] \n",
    "        final_chunks_df.drop(columns=['Initial_Timestamp', \"Current_Timestamp\"], inplace=True)\n",
    "        # print(final_chunks_df)\n",
    "        \n",
    "\n",
    "        # Extract metadata for the chunks\n",
    "        llama_70b_responses = extract_metadata(nlp_pipeline, final_chunks_df)\n",
    "        # Ensure that the response DataFrame contains metadata columns\n",
    "        if not llama_70b_responses.empty:\n",
    "            # Merge chunk outputs into a single row \n",
    "            merged_metadata = llama_70b_responses.apply(lambda col: ' '.join(col.dropna().astype(str)))\n",
    "            \n",
    "            for column in merged_metadata.index:\n",
    "                unique_values = set([value.strip() for value in merged_metadata[column].strip().split(\",\")])\n",
    "                list_unique_values = list(filter(None, unique_values))\n",
    "                merged_metadata[column] = \" | \".join(list_unique_values)\n",
    "\n",
    "            # Add filename for reference\n",
    "            # merged_metadata[\"Filename\"] = filename  \n",
    "\n",
    "            # Append to list\n",
    "            all_metadata.append(merged_metadata)\n",
    "        else:\n",
    "            print(f\"No metadata extracted from {filename}\")\n",
    "         \n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert list of metadata rows into a single DataFrame\n",
    "final_metadata_df = pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a643c5d7-2dbe-406b-8332-b9061b7589b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speaker</th>\n",
       "      <th>FÃ¼gen Sie nach jedem Wert in Klammern den entsprechenden 00</th>\n",
       "      <th>Transkript</th>\n",
       "      <th>Metadaten columns to be present</th>\n",
       "      <th>NAME</th>\n",
       "      <th>JAHRGANG</th>\n",
       "      <th>ORT</th>\n",
       "      <th>GESCHLECHT</th>\n",
       "      <th>BERUF</th>\n",
       "      <th>VAT_JG</th>\n",
       "      <th>VAT_KONFESSION</th>\n",
       "      <th>VAT_HERKUN</th>\n",
       "      <th>VAT_SCHULE</th>\n",
       "      <th>VAT_AUSBIL</th>\n",
       "      <th>VAT_STAND</th>\n",
       "      <th>VAT_POLOR</th>\n",
       "      <th>Der Tagesablauf, ja halt wecken, wie frÃ¼her im Landjahr auch, um 6 Uhr wurde geweckt, im Winter um 7 Uhr, und dann war normalerweise FrÃ¼hsport. Entweder sind wir ..., im Sommer haben wir drauÃŸen FrÃ¼hsport gemacht und im Winter im ..., also wir haben schon mal einen Lauf gemacht oder so FreiÃ¼bungen und im Winter, dann haben wir das halt im Hof nur so FreiÃ¼bungen gemacht und keine LÃ¤ufe. Das hat so vielleicht sagen wir mal 10 Minuten bis eine Viertelstunde. Und dann gingâ€™s zum Waschen, in die WaschrÃ¤ume, in die DuschrÃ¤ume und anschlieÃŸend war FrÃ¼hstÃ¼ck. Ja nach dem FrÃ¼hstÃ¼ck wurde dann die Arbeit verteilt. Die ersten 4 Wochen blieb man halt im Lager. Da wurde man vertraut gemacht mit eben all den Dingen, mit den Tageseinteilungen. Und dann wurde eingeteilt zum Hausdienst, also, sagen wir mal, sauber machen oder WaschkÃ¼chengruppe, BÃ¼gelgruppe oder KÃ¼che und auch Verwaltungsarbeiten. Also im BÃ¼ro mussten wir auch helfen. Eben, sagen wir mal, es wurde jeder getestet nach seinen FÃ¤higkeiten und auch mehr oder weniger denn da eingesetzt. Er musste zwar alles lernen, er kam Ã¼berall mit rein aber, ich meine, viele schimpfen auf den Arbeitsdienst und sagen, das war Mist, das war BlÃ¶dsinn. Vielleicht lagâ€™s an der FÃ¼hrung aber, wie gesagt, wir hatten das GlÃ¼ck, wir hatten ne wirklich, also sehr gute FÃ¼hrerin, die Ã¤uÃŸerst korrekt war. Also wirklich Ã¤uÃŸerst korrekt. Die weder Entgleisungen duldete, noch sich selbst, sagen wir mal, irgendwie gehen lieÃŸ. Die auch sah, wenn irgendwelche Ungerechtigkeiten waren. Zum Beispiel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INT_AH | IP_FA</td>\n",
       "      <td>07:22.00 - 00:16:30.00 hinzu. | 00:06.00 - 00:...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>%%% (00:04:11.00 - 00:07:23.00) | MÃ¼lheim an d...</td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>BÃ¼ro (00:04:11.00 - 00:07:23.00) | %%% (00:07:...</td>\n",
       "      <td>42 (00:07:22.00 - 00:16:30.00) | %%% (00:04:11...</td>\n",
       "      <td>%%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...</td>\n",
       "      <td>%%% (00:07:22.00 | %%% (00:04:11.00 - 00:07:23...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ans Bett gebracht | entgleiste die Lokomotive ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Speaker FÃ¼gen Sie nach jedem Wert in Klammern den entsprechenden 00  \\\n",
       "0  INT_AH | IP_FA  07:22.00 - 00:16:30.00 hinzu. | 00:06.00 - 00:...            \n",
       "\n",
       "  Transkript Metadaten columns to be present  \\\n",
       "0                                              \n",
       "\n",
       "                                                NAME  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                            JAHRGANG  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                                 ORT  \\\n",
       "0  %%% (00:04:11.00 - 00:07:23.00) | MÃ¼lheim an d...   \n",
       "\n",
       "                                          GESCHLECHT  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                               BERUF  \\\n",
       "0  BÃ¼ro (00:04:11.00 - 00:07:23.00) | %%% (00:07:...   \n",
       "\n",
       "                                              VAT_JG  \\\n",
       "0  42 (00:07:22.00 - 00:16:30.00) | %%% (00:04:11...   \n",
       "\n",
       "                                      VAT_KONFESSION  \\\n",
       "0  %%% (00:07:22.00 - 00:16:30.00) | %%% (00:04:1...   \n",
       "\n",
       "                                          VAT_HERKUN VAT_SCHULE VAT_AUSBIL  \\\n",
       "0  %%% (00:07:22.00 | %%% (00:04:11.00 - 00:07:23...                         \n",
       "\n",
       "  VAT_STAND VAT_POLOR  \\\n",
       "0                       \n",
       "\n",
       "  Der Tagesablauf, ja halt wecken, wie frÃ¼her im Landjahr auch, um 6 Uhr wurde geweckt, im Winter um 7 Uhr, und dann war normalerweise FrÃ¼hsport. Entweder sind wir ..., im Sommer haben wir drauÃŸen FrÃ¼hsport gemacht und im Winter im ..., also wir haben schon mal einen Lauf gemacht oder so FreiÃ¼bungen und im Winter, dann haben wir das halt im Hof nur so FreiÃ¼bungen gemacht und keine LÃ¤ufe. Das hat so vielleicht sagen wir mal 10 Minuten bis eine Viertelstunde. Und dann gingâ€™s zum Waschen, in die WaschrÃ¤ume, in die DuschrÃ¤ume und anschlieÃŸend war FrÃ¼hstÃ¼ck. Ja nach dem FrÃ¼hstÃ¼ck wurde dann die Arbeit verteilt. Die ersten 4 Wochen blieb man halt im Lager. Da wurde man vertraut gemacht mit eben all den Dingen, mit den Tageseinteilungen. Und dann wurde eingeteilt zum Hausdienst, also, sagen wir mal, sauber machen oder WaschkÃ¼chengruppe, BÃ¼gelgruppe oder KÃ¼che und auch Verwaltungsarbeiten. Also im BÃ¼ro mussten wir auch helfen. Eben, sagen wir mal, es wurde jeder getestet nach seinen FÃ¤higkeiten und auch mehr oder weniger denn da eingesetzt. Er musste zwar alles lernen, er kam Ã¼berall mit rein aber, ich meine, viele schimpfen auf den Arbeitsdienst und sagen, das war Mist, das war BlÃ¶dsinn. Vielleicht lagâ€™s an der FÃ¼hrung aber, wie gesagt, wir hatten das GlÃ¼ck, wir hatten ne wirklich, also sehr gute FÃ¼hrerin, die Ã¤uÃŸerst korrekt war. Also wirklich Ã¤uÃŸerst korrekt. Die weder Entgleisungen duldete, noch sich selbst, sagen wir mal, irgendwie gehen lieÃŸ. Die auch sah, wenn irgendwelche Ungerechtigkeiten waren. Zum Beispiel  \n",
       "0  ans Bett gebracht | entgleiste die Lokomotive ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bb53937-9e0b-4fa3-89c9-d6385c8a2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_df.to_csv(\"metadata_results.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
