{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d49b97-55a8-40f7-9861-f450ab0cb16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqqq pip --progress-bar off\n",
    "!pip install -qqq groq==0.13.0 --progress-bar off\n",
    "!pip install -qqq python-dotenv==1.0.1 --progress-bar off\n",
    "!pip install groq\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf0abf-f7bd-4f31-abe7-9350fce54151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6436c012-5008-435b-89b5-658b701e9855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_transcript(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # Ensure 'Transkript' column has no NaN values\n",
    "    data['Transkript'] = data['Transkript'].fillna(\"\").astype(str)\n",
    "\n",
    "    # Chunking logic\n",
    "    chunked_data = []\n",
    "    current_speaker = None\n",
    "    current_text = \"\"\n",
    "    initial_timestamp = \"\"\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        speaker = row['Sprecher']\n",
    "        transcript = row['Transkript']\n",
    "        timestamp = row['Timecode'] \n",
    "\n",
    "        if speaker != current_speaker:\n",
    "            # Save previous chunk if exists\n",
    "            if current_speaker is not None:\n",
    "                chunked_data.append({\n",
    "                    'Speaker': current_speaker,\n",
    "                    'Transcript': current_text.strip(),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : timestamp\n",
    "                })\n",
    "            \n",
    "            # Start a new chunk\n",
    "            current_speaker = speaker\n",
    "            current_text = transcript\n",
    "            initial_timestamp = timestamp \n",
    "        else:\n",
    "            # Continue appending to the same speaker's chunk\n",
    "            current_text += \" \" + transcript if isinstance(transcript, str) else \"\"\n",
    "\n",
    "    # Save the last chunk\n",
    "    if current_speaker is not None:\n",
    "        chunked_data.append({\n",
    "            'Speaker': current_speaker,\n",
    "            'Transcript': current_text.strip(),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    chunked_df = pd.DataFrame(chunked_data)\n",
    "    \n",
    "    return chunked_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc851cb5-4772-4693-a60f-fe27baf3425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentence(chunked_df: pd.DataFrame, min_tokens=256, max_tokens=512) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Further splits chunks by sentence while ensuring each chunk is within a token range.\n",
    "\n",
    "    Args:\n",
    "        chunked_df (pd.DataFrame): Input DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "        min_tokens (int): Minimum number of tokens per chunk.\n",
    "        max_tokens (int): Maximum number of tokens per chunk.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with sentence-based chunked transcripts.\n",
    "    \"\"\"\n",
    "\n",
    "    # Function to count tokens (approximate, assuming 1 word ≈ 1.2 tokens)\n",
    "    def count_tokens(text):\n",
    "        return len(text.split()) * 1.2  # Rough estimate\n",
    "\n",
    "    # Initialize list for final merged chunks\n",
    "    merged_chunks = []\n",
    "    temp_chunk = []\n",
    "    temp_token_count = 0\n",
    "    speaker = None\n",
    "    initial_timestamp = \"\"\n",
    "    final_timestamp = \"\"\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in chunked_df.iterrows():\n",
    "        sentence = row['Transcript']\n",
    "        sentence_tokens = count_tokens(sentence)\n",
    "\n",
    "        # If adding this chunk keeps us within MAX_TOKENS\n",
    "        if temp_token_count + sentence_tokens <= max_tokens:\n",
    "            if not temp_chunk:\n",
    "                speaker = row['Speaker']  # Store speaker only for new chunks\n",
    "                initial_timestamp = row['Initial_Timestamp']\n",
    "            temp_chunk.append(sentence)\n",
    "            temp_token_count += sentence_tokens\n",
    "        else:\n",
    "            # Save the previous chunk before starting a new one\n",
    "            if temp_chunk:\n",
    "                merged_chunks.append({\n",
    "                    'Speaker': speaker,\n",
    "                    'Transcript': \" \".join(temp_chunk),\n",
    "                    'Initial_Timestamp' : initial_timestamp,\n",
    "                    'Current_Timestamp' : row['Current_Timestamp']\n",
    "                })\n",
    "\n",
    "            # Start a new chunk with the current sentence\n",
    "            temp_chunk = [sentence]\n",
    "            temp_token_count = sentence_tokens\n",
    "            speaker = row['Speaker']\n",
    "            initial_timestamp = row['Initial_Timestamp']\n",
    "            final_timestamp = row['Current_Timestamp']\n",
    "\n",
    "    # Save last chunk if any content remains\n",
    "    if temp_chunk:\n",
    "        merged_chunks.append({\n",
    "            'Speaker': speaker,\n",
    "            'Transcript': \" \".join(temp_chunk),\n",
    "            'Initial_Timestamp' : initial_timestamp,\n",
    "            'Current_Timestamp' : final_timestamp\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    final_merged_df = pd.DataFrame(merged_chunks)\n",
    "    \n",
    "    return final_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e744d3c1-7d68-4abc-bfec-e32b54bed9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import pandas as pd \n",
    "\n",
    "api_key = {your api_key}\n",
    "\n",
    "# Initialize the Groq client\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "print(\"Groq client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed53c67-a80b-4447-b24b-ec542bc7e00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata(client, model_name: str, chunks: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metadata from multiple chunks of a German transcript using the specified model.\n",
    "    \n",
    "    Args:\n",
    "        client: The Groq client object.\n",
    "        model_name: The name of the model to use (e.g., 'llama3-8b-8192').\n",
    "        chunks: A DataFrame with 'Speaker' and 'Transcript' columns.\n",
    "    \n",
    "    Returns:\n",
    "        A DataFrame containing the extracted metadata for all chunks.\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "    num_chunks = len(chunks)\n",
    "\n",
    "    for i, row in chunks.iterrows():  # Iterate over DataFrame rows\n",
    "        speaker = row[\"Speaker\"]\n",
    "        transcript = row[\"Transcript\"]\n",
    "        timestamp = row['Timestamp']\n",
    "\n",
    "        print(f\"Processing chunk {i+1}/{len(chunks)} for Speaker: {speaker}...\")\n",
    "\n",
    "        # Eingabeaufforderung zur Metadatenextraktion\n",
    "        prompt = f\"\"\"\n",
    "\n",
    "        Extrahieren Sie die folgenden Informationen aus dem Transkript und geben Sie die einsilbige Antwort auf Deutsch ein.  \n",
    "        Wenn die Antwort nicht gefunden wurde, geben Sie stattdessen %%% zurück.  \n",
    "        \n",
    "        Wenn es mehrere Antworten gibt, wählen Sie eine einzelne geeignete Antwort entsprechend dem Spaltennamen und der unten angegebenen Beschreibung.  \n",
    "        Fügen Sie nach jedem Wert in Klammern den entsprechenden {timestamp} hinzu.  \n",
    "        Wenn die generierten Antworten auf kontextuellem Verständnis basieren und nicht im {transcript} enthalten sind, fügen Sie den {timestamp} nicht hinzu.  \n",
    "        Fügen Sie außerdem den {timestamp} nicht hinzu, wenn die Antwort %%% ist oder nicht gefunden wurde. \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        NAME: Der vollständige Name der Person (Vor- und Nachname, falls verfügbar).\n",
    "        JAHRGANG: Das Geburtsjahr der Person.\n",
    "        ORT: Der Geburtsort der Person.\n",
    "        GESCHLECHT: Das Geschlecht der Person (z.B. männlich, weiblich, divers).\n",
    "        BERUF: Der aktuelle Beruf der Person.\n",
    "\n",
    "        VAT_JG: Geburtsjahr des Vaters.\n",
    "        VAT_KONFESSION: Religion des Vaters.\n",
    "        VAT_HERKUN: Herkunft des Vaters.\n",
    "        VAT_SCHULE: Schulbildung des Vaters.\n",
    "        VAT_AUSBIL: Ausbildung des Vaters.\n",
    "        VAT_STAND: Beruflicher Status des Vaters.\n",
    "        VAT_POLOR: Politische Orientierung des Vaters.\n",
    "\n",
    "        DATENBOGEN: Während des Interviews verwendetes Datenblatt oder Formular.\n",
    "        KURZBESCHR: Kurzbeschreibung des Interviews.\n",
    "        TITEL: Titel des Dokuments oder der Aufzeichnung.\n",
    "        DAUER: Dauer des Interviews.\n",
    "        INTERVIEWE: Name des Interviewers.\n",
    "        TIPPER: Schreibkraft oder Person, die das Interview transkribiert hat.\n",
    "        Segmentierung: Segmentierung (Kategorien oder Abschnitte innerhalb des Interviews).\n",
    "        DATUM1, DATUM2, DATUM3: Wichtige Daten im Zusammenhang mit dem Interview.\n",
    "\n",
    "\n",
    "        STRASSE: Straßenadresse.\n",
    "        PLZ: Postleitzahl.\n",
    "        TELEFON: Telefonnummer.\n",
    "        \n",
    "        GRUPPE: Gruppenzugehörigkeit (z. B. politische, soziale oder kulturelle Gruppe).\n",
    "        BERUF: Beruf oder Tätigkeit.\n",
    "        HEUT_FAMST: Aktueller Familienstand (z. B. ledig, verheiratet, verwitwet).\n",
    "\n",
    "        FOTOS: Mit dem Datensatz verbundene Fotos.\n",
    "        DOKUMENTE: Mit dem Datensatz verbundene Dokumente.\n",
    "        VHS, DVD: Für die Aufzeichnung verwendete Medienformate (falls zutreffend).\n",
    "        IBM-Server: Ob auf einem IBM-Server gespeichert.\n",
    "        Cloud: Ob Daten in der Cloud gespeichert sind.\n",
    "        ORIGCASSET, CASSKOPIEN: Originalkassette und Kopien.\n",
    "        FESTPLATTE: Festplattenspeicherort.\n",
    "        Dig Audiofiles: Mit dem Datensatz verbundene digitale Audiodateien.\n",
    "\n",
    "        - Schulabsch: Schulbildungsniveau.\n",
    "        - ABGEBROCHE: Abgebrochene Ausbildung.\n",
    "        - WEITERBILD: Weiterbildung oder Ausbildung.\n",
    "        - AUSBILDUNG: Berufsausbildung.\n",
    "        - STAND: Aktueller beruflicher Status.\n",
    "        - BERUFSWECH: Berufswechsel (falls zutreffend).\n",
    "        - WANN_WECHS: Wann der Berufswechsel stattfand.\n",
    "        - AUFABSTIEG: Karriereaufstieg oder -rückgang.\n",
    "        - BERUFSBEGI: Berufseinstiegsjahr.\n",
    "        - BERUFSENDE: Berufsabschlussjahr.\n",
    "        - NICHTERWER: Nichterwerbstätiger Status (falls zutreffend).\n",
    "        - GRÜNDE: Gründe für Nichtbeschäftigung.\n",
    "\n",
    "        - FAM_STAND: Familienstand.\n",
    "        - HEIRAT1JHR: Ehejahre (falls zutreffend).\n",
    "          HEIRAT2JHR: Ehejahre (falls zutreffend).\n",
    "          HEIRAT3JHR: Ehejahre (falls zutreffend).\n",
    "        - SCHEID1JHR: Scheidungsjahre\n",
    "          SCHEID2JHR: Scheidungsjahre.\n",
    "        - VERWIT1JHR:Witwenjahre.\n",
    "          VERWIT2JHR: Witwenjahre.\n",
    "        - KINDERZAHL: Anzahl der Kinder.\n",
    "        - GEB_JAHR1:Geburtsjahre der Kinder.\n",
    "          GEB_JAHR2:Geburtsjahre der Kinder.\n",
    "          GEB_JAHR_L: Geburtsjahre der Kinder.\n",
    "        \n",
    "\n",
    "        - POLOR_HEUT: Aktuelle politische Orientierung.\n",
    "        - POL_KONVER: Politische Konversion (sofern vorhanden).\n",
    "        - POL_ORIENT1:\n",
    "          VON_BIS_1: Erste politische Orientierung und Dauer.\n",
    "        - POL_ORIENT2:\n",
    "          VON_BIS_2: Zweite politische Orientierung und Dauer.\n",
    "        - GEW_VERBAN:\n",
    "          VON_BIS_GV: Mitgliedschaft in Organisationen oder Gewerkschaften und Dauer.\n",
    "\n",
    "\n",
    "        - JUGENDORG1:\n",
    "          VON_BIS_J1: Erste Jugendorganisation und Dauer.\n",
    "        - JUGENDORG2:\n",
    "          VON_BIS_J2: Zweite Jugendorganisation und Dauer.\n",
    "        - NS_ORGAN_1:\n",
    "          VON_BISNS1: Erste NS-nahe Organisation und Dauer.\n",
    "        - NS_ORGAN_2, VON_BISNS2: Zweite mit dem Nazismus verbundene Organisation und Dauer.\n",
    "        - RAD_KLV_DV, VON_BISRAD: Anderes historisches Engagement und Dauer.\n",
    "\n",
    "\n",
    "        - KRIEGSTEIL, VON_BIS_KR: Kriegsteilnahme und -dauer.\n",
    "        - BES_BERICH: Kriegsbezogene Gebiete oder Erfahrungen.\n",
    "\n",
    "        - MUTT_JG: Geburtsjahr der Mutter.\n",
    "        - MUTT_KONFESSION: Religion der Mutter.\n",
    "        - MUTT_HERKU: Herkunft der Mutter.\n",
    "        - MUTT_SCHUL: Schulbildung der Mutter.\n",
    "        - MUTT_AUSBI: Ausbildung der Mutter.\n",
    "        - MUTT_STAND: Beruflicher Status der Mutter.\n",
    "        - MUTT_POLOR: Politische Orientierung der Mutter.\n",
    "\n",
    "        - VAT_JG, PART_JG: Geburtsjahr des Vaters/Partners.\n",
    "        - VAT_KONFESSION, PART_KONFESSION: Religion.\n",
    "        - VAT_HERKUN, PART_HERKU: Herkunft.\n",
    "        - VAT_SCHULE, PART_SCHUL: Bildung.\n",
    "        - VAT_AUSBIL, PART_AUSBI: Schulung.\n",
    "        - VAT_STAND, PART_STAND: Professioneller Status.\n",
    "        - VAT_POLOR, PART_POLOR: Politische Ausrichtung.\n",
    "        - PART_BERUF: Beruf des Partners.\n",
    "        \n",
    "        - PART_PKONV: Politische Bekehrung des Partners.\n",
    "        - PART_ENGAG: Soziales/politisches Engagement des Partners.\n",
    "        - KRIT10: Anmerkungen oder Kritik.\n",
    "\n",
    "\n",
    "        Transkript: \n",
    "        {transcript}\n",
    "        \n",
    "        Metadaten:\n",
    "        STANDORT:\n",
    "        ARCHIV ID:\n",
    "        PROBANDNR:\n",
    "        DOK_ART:\n",
    "        ARCHIVORT:\n",
    "        PROVENIENZ:\n",
    "        SPERRUNG:\n",
    "        ENTSTZEIT:\n",
    "        ZEITUMFANG 1:\n",
    "        NAME:\n",
    "        VORNAME:\n",
    "        ORT:\n",
    "        FELD1:\n",
    "        PSEUDONYM:\n",
    "        GESCHLECHT:\n",
    "        JAHRGANG:\n",
    "        IPV:\n",
    "        DATENBOGEN:\n",
    "        KURZBESCHR:\n",
    "        TITEL:\n",
    "        STRASSE:\n",
    "        PLZ:\n",
    "        TELEFON:\n",
    "        GRUPPE:\n",
    "        BERUF:\n",
    "        HEUT_FAMST:\n",
    "        INTERVIEWE:\n",
    "        TIPPER:\n",
    "        SEGMENTIERUNG:\n",
    "        DATUM1:\n",
    "        DATUM2:\n",
    "        DATUM3:\n",
    "        DAUER:\n",
    "        ONLINE:\n",
    "        AUSDRUCKSART:\n",
    "        UNKAUSDRUC:\n",
    "        KORRAUSDRU:\n",
    "        SCHLAGWORT:\n",
    "        KURZBIOGRA:\n",
    "        KURZPROTOK:\n",
    "        FOTOS:\n",
    "        DOKUMENTE:\n",
    "        VHS:\n",
    "        DVD:\n",
    "        IBM SERVER:\n",
    "        CLOUD:\n",
    "        FORMAT CLOUD:\n",
    "        DV:\n",
    "        BETA:\n",
    "        ORIGCASSET:\n",
    "        CASSKOPIEN:\n",
    "        FESTPLATTE:\n",
    "        DIG AUDIOFILES:\n",
    "        KONF_HEUTE:\n",
    "        KONVERSION:\n",
    "        WANN_KONV:\n",
    "        HERKUNFT:\n",
    "        WANN_ZUGEZ:\n",
    "        GESCHWISTE:\n",
    "        SCHULABSCH:\n",
    "        ABGEBROCHE:\n",
    "        WEITERBILD:\n",
    "        AUSBILDUNG:\n",
    "        STAND:\n",
    "        WIRTSCHBER:\n",
    "        BERUFSWECH:\n",
    "        WANN_WECHS:\n",
    "        AUFABSTIEG:\n",
    "        BERUFSBEGI:\n",
    "        BERUFSENDE:\n",
    "        NICHTERWER:\n",
    "        GRÜNDE:\n",
    "        VON_BIS:\n",
    "        ARBEITSLOS:\n",
    "        VON_BIS_AL:\n",
    "        FAM_STAND:\n",
    "        HEIRAT1JHR:\n",
    "        HEIRAT2JHR:\n",
    "        HEIRAT3JHR:\n",
    "        SCHEID1JHR:\n",
    "        SCHEID2JHR:\n",
    "        VERWIT1JHR:\n",
    "        VERWIT2JHR:\n",
    "        KINDERZAHL:\n",
    "        GEB_JAHR1:\n",
    "        GEB_JAHR2:\n",
    "        GEB_JAHR_L:\n",
    "        AUFABKIND:\n",
    "        POLOR_HEUT:\n",
    "        POL_KONVER:\n",
    "        POLORIENT1:\n",
    "        VON_BIS_1:\n",
    "        POLORIENT2:\n",
    "        VON_BIS_2:\n",
    "        GEW_VERBAN:\n",
    "        VON_BIS_GV:\n",
    "        JUGENDORG1:\n",
    "        VON_BIS_J1:\n",
    "        JUGENDORG2:\n",
    "        VON_BIS_J2:\n",
    "        NS_ORGAN_1:\n",
    "        VON_BISNS1:\n",
    "        NS_ORGAN_2:\n",
    "        VON_BISNS2:\n",
    "        RAD_KLV_DV:\n",
    "        VON_BISRAD:\n",
    "        KRIEGSTEIL:\n",
    "        VON_BIS_KR:\n",
    "        BES_BERICH:\n",
    "        MUTT_JG:\n",
    "        MUTT_KONFESSION:\n",
    "        MUTT_HERKU:\n",
    "        MUTT_SCHUL:\n",
    "        MUTT_AUSBI:\n",
    "        MUTT_STAND:\n",
    "        MUTT_POLOR:\n",
    "        VAT_JG:\n",
    "        VAT_KONFESSION:\n",
    "        VAT_HERKUN:\n",
    "        VAT_SCHULE:\n",
    "        VAT_AUSBIL:\n",
    "        VAT_STAND:\n",
    "        VAT_POLOR:\n",
    "        PART_JG:\n",
    "        PART_KONFESSION:\n",
    "        PART_HERKU:\n",
    "        PART_SCHUL:\n",
    "        PART_AUSBI:\n",
    "        PART_STAND:\n",
    "        PART_BERUF:\n",
    "        PART_POLOR:\n",
    "        PART_PKONV:\n",
    "        PART_ENGAG:\n",
    "        KRIT10:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        Fügen Sie keine Präambel ein.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Generate response using the Groq client\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0,\n",
    "        )\n",
    "\n",
    "        # Extract metadata from the response\n",
    "        metadata = response.choices[0].message.content\n",
    "\n",
    "        # Parse metadata into a dictionary\n",
    "        extracted_metadata = {\"Speaker\": speaker}  # Store speaker info\n",
    "        for line in metadata.split(\"\\n\"):\n",
    "            if \":\" in line:\n",
    "                key, value = line.split(\":\", 1)\n",
    "                extracted_metadata[key.strip()] = value.strip()\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        if i < num_chunks - 1:  # If not the last iteration\n",
    "            extracted_metadata = {k: v.strip() + \",\" if v else v.strip() for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Append metadata for this chunk\n",
    "        all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    return pd.DataFrame(all_metadata)\n",
    "\n",
    "\n",
    "        # if i < num_chunks - 1:  # If not the last iteration\n",
    "        #     for k, v in extracted_metadata.items():\n",
    "        #         if v:  # If the value is not empty\n",
    "        #             existing_values = set([extracted_metadata[k]][0].strip().split(\" \"))  # Convert existing values to a set\n",
    "        #             # existing_values.add(v)  # Add new value (set avoids duplicates)\n",
    "        #             extracted_metadata[k] = \" | \".join(existing_values)  # Join back without duplicates\n",
    "\n",
    "\n",
    "\n",
    "        # # Append metadata for this chunk\n",
    "        # all_metadata.append(extracted_metadata)\n",
    "\n",
    "\n",
    "    #     # Check for duplicates column-wise\n",
    "    #     is_duplicate = False\n",
    "    #     for prev_metadata in all_metadata:\n",
    "    #         for key in extracted_metadata.keys():\n",
    "    #             if key in prev_metadata and extracted_metadata[key] == prev_metadata[key]:  \n",
    "    #                 is_duplicate = True\n",
    "    #                 continue\n",
    "    #         if is_duplicate:\n",
    "    #             break  \n",
    "\n",
    "    #     # If the entire metadata row is not a duplicate, append it\n",
    "    #     if not is_duplicate:\n",
    "    #         all_metadata.append(extracted_metadata)\n",
    "\n",
    "    # Convert metadata list into a DataFrame\n",
    "    # return pd.DataFrame(all_metadata)\n",
    "\n",
    "        # Remove \"%%%%\" values before storing\n",
    "        # extracted_metadata = {k: (v if v != \"%%%\" else \"\") for k, v in extracted_metadata.items()}\n",
    "\n",
    "        # Check for duplicates column-wise\n",
    "    #     cleaned_metadata = {\"Speaker\": speaker}  # Keep speaker info\n",
    "    #     for key, value in extracted_metadata.items():\n",
    "    #         if key == \"Speaker\":\n",
    "    #             continue  # Skip speaker key check\n",
    "    #         is_duplicate = any(value == prev_row.get(key, None) for prev_row in all_metadata)\n",
    "            \n",
    "    #         if not is_duplicate:  # Add only if value is unique\n",
    "    #             cleaned_metadata[key] = value\n",
    "\n",
    "    #         for key in cleaned_metadata.keys():\n",
    "    #             values = [v for v in cleaned_metadata[key].split(\" | \") if v]  # Remove empty values\n",
    "    #             cleaned_metadata[key] = \" | \".join(values) \n",
    "\n",
    "\n",
    "    #     # If there’s any new unique metadata, add it\n",
    "    #     if len(cleaned_metadata) > 1:  \n",
    "    #         all_metadata.append(cleaned_metadata)\n",
    "\n",
    "    # # Convert metadata list into a DataFrame\n",
    "    # return pd.DataFrame(all_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944efa9a-7293-4fad-8106-d4841c2ac707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "folder_path = \"Transcripts ADG0001-10\"\n",
    "MODEL = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "# List to store metadata for all files\n",
    "all_metadata = []\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    if os.path.isfile(file_path) & filename.endswith(\".csv\"):\n",
    "        print(f\"\\nProcessing file: {filename}\")\n",
    "        input_data = pd.read_csv(file_path, sep=None, engine='python')\n",
    "        speaker_chunks_df = chunk_transcript(input_data)  # Stores speaker-based chunks\n",
    "        final_chunks_df = chunk_by_sentence(speaker_chunks_df)\n",
    "        final_chunks_df['Timestamp'] = final_chunks_df['Initial_Timestamp'] + \" - \" + final_chunks_df['Current_Timestamp'] \n",
    "        final_chunks_df.drop(columns=['Initial_Timestamp', \"Current_Timestamp\"], inplace=True)\n",
    "        # print(final_chunks_df)\n",
    "        \n",
    "\n",
    "        # Extract metadata for the chunks\n",
    "        llama_70b_responses = extract_metadata(client,MODEL , final_chunks_df)\n",
    "\n",
    "        # Ensure that the response DataFrame contains metadata columns\n",
    "        if not llama_70b_responses.empty:\n",
    "            # Merge chunk outputs into a single row \n",
    "            merged_metadata = llama_70b_responses.apply(lambda col: ' '.join(col.dropna().astype(str)))\n",
    "            \n",
    "            for column in merged_metadata.index:\n",
    "                unique_values = set([value.strip() for value in merged_metadata[column].strip().split(\",\")])\n",
    "                list_unique_values = list(filter(None, unique_values))\n",
    "                merged_metadata[column] = \" | \".join(list_unique_values)\n",
    "\n",
    "            # Add filename for reference\n",
    "            # merged_metadata[\"Filename\"] = filename  \n",
    "\n",
    "            # Append to list\n",
    "            all_metadata.append(merged_metadata)\n",
    "        else:\n",
    "            print(f\"No metadata extracted from {filename}\")\n",
    "         \n",
    "        time.sleep(0.5)\n",
    "\n",
    "# Convert list of metadata rows into a single DataFrame\n",
    "final_metadata_df = pd.DataFrame(all_metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9cf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb53937-9e0b-4fa3-89c9-d6385c8a2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_metadata_df.to_csv(\"metadata_resultsd.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
